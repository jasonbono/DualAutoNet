{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import roc_curve, auc, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "#ucimlrepo is for the data\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "\n",
    " # Fetch the Breast Cancer Wisconsin dataset from UCI repository\n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)\n",
    "\n",
    "# Get the features (X) and targets (y) as pandas DataFrames\n",
    "X = breast_cancer_wisconsin_diagnostic.data.features\n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets\n",
    "\n",
    "# Print columns to verify the structure\n",
    "#print(\"Feature columns:\\n\", X.columns)\n",
    "#print(\"Target columns:\\n\", y.columns)\n",
    "\n",
    "y = y.iloc[:, 0]  # extract the Series\n",
    "\n",
    "# Map the diagnosis labels: 1 for malignant ('M'), 0 for benign ('B')\n",
    "y = y.map({'M': 1, 'B': 0})  # Adjust this mapping as per the dataset's structure\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the input dimension\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# take a subset of the training data that only has benign samples\n",
    "X_train_benign = X_train[y_train == 0]\n",
    "\n",
    "# input dimension for the benign model\n",
    "input_dim_benign = X_train_benign.shape[1]\n",
    "\n",
    "#take a subset of the training data that only has malignant samples\n",
    "X_train_malignant = X_train[y_train == 1]\n",
    "\n",
    "# input dimension for the malignant model\n",
    "input_dim_malignant = X_train_malignant.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15/15 [==============================] - 0s 730us/step - loss: 1.1485\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 0s 713us/step - loss: 1.0573\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 13:42:38.585129: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 678us/step - loss: 1.0057\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 0s 608us/step - loss: 0.9639\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 0s 693us/step - loss: 0.9129\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 0s 734us/step - loss: 0.8451\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 0s 707us/step - loss: 0.7723\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 0s 746us/step - loss: 0.7019\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 0s 690us/step - loss: 0.6485\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 0s 690us/step - loss: 0.6074\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 0s 932us/step - loss: 0.7271\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 0s 917us/step - loss: 0.6977\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 0s 969us/step - loss: 0.6744\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 0s 741us/step - loss: 0.6526\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 0s 783us/step - loss: 0.6299\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 0s 813us/step - loss: 0.6067\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 0s 693us/step - loss: 0.5822\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 0s 786us/step - loss: 0.5577\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 0s 703us/step - loss: 0.5302\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 0s 677us/step - loss: 0.5043\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 0s 859us/step - loss: 1.5193\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 0s 716us/step - loss: 1.4739\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 0s 835us/step - loss: 1.4362\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 0s 772us/step - loss: 1.4008\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 0s 821us/step - loss: 1.3678\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 0s 705us/step - loss: 1.3322\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 0s 790us/step - loss: 1.2962\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 0s 696us/step - loss: 1.2575\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 0s 715us/step - loss: 1.2137\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 0s 750us/step - loss: 1.1650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3152b25b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to define the shared model arhitecture \n",
    "def create_autoencoder(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(14, activation='relu', input_shape=(input_dim,)),\n",
    "        Dense(7, activation='relu'),\n",
    "        Dense(14, activation='relu'),\n",
    "        Dense(input_dim, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create the standard  autoencoder models\n",
    "autoencoder_normal = create_autoencoder(input_dim)\n",
    "autoencoder_benign = create_autoencoder(input_dim_benign)\n",
    "autoencoder_malignant = create_autoencoder(input_dim_malignant)\n",
    "\n",
    "\n",
    "# Compile the standard models witht the adam optimizer\n",
    "autoencoder_normal.compile(optimizer='adam', loss='mean_squared_error')\n",
    "autoencoder_benign.compile(optimizer='adam', loss='mean_squared_error')\n",
    "autoencoder_malignant.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "# train the standard models\n",
    "autoencoder_normal.fit(X_train, X_train, epochs=10, batch_size=32,verbose=1)\n",
    "autoencoder_benign.fit(X_train_benign, X_train_benign, epochs=10, batch_size=32,verbose=1)\n",
    "autoencoder_malignant.fit(X_train_malignant, X_train_malignant, epochs=10, batch_size=32,verbose=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the custom loss fuction for the modified model\n",
    "from base64 import standard_b64decode\n",
    "\n",
    "\n",
    "def custom_loss_pos(X_batch, X_pred, labels_batch):\n",
    "    # Compute the MSE between the true input and the reconstructed input\n",
    "    mse = tf.reduce_mean(tf.square(X_batch - X_pred), axis=1)\n",
    "\n",
    "    # standard batch-level MSE\n",
    "    standard_batch_level_mse = tf.reduce_mean(mse)\n",
    "\n",
    "    # standard batch-level MSE using reduce_sum\n",
    "    standard_batch_level_mse_reduce_sum = tf.reduce_sum(mse) / len(labels_batch)\n",
    "\n",
    "    # Create a mask to include only those points where labels == 0\n",
    "    mask = tf.cast(labels_batch == 0, dtype=tf.float32)\n",
    "    mask = tf.reshape(mask, [-1])  # Flatten to shape [batch_size]\n",
    "\n",
    "    # Create the inverse mask\n",
    "    inverse_mask = 1 - mask\n",
    "\n",
    "    # Apply the mask to the MSE\n",
    "    masked_mse = mse * mask\n",
    "\n",
    "    # Apply the inverse mask to the MSE\n",
    "    inverse_masked_mse = mse * inverse_mask\n",
    "\n",
    "    # Calculate the masked batch-level MSE\n",
    "    masked_numerator = tf.reduce_sum(masked_mse)\n",
    "    masked_denominator = tf.reduce_sum(mask)\n",
    "    masked_batch_level_mse = masked_numerator / masked_denominator\n",
    "\n",
    "    # Calculate the inverse masked batch-level MSE\n",
    "    inverse_masked_numerator = tf.reduce_sum(inverse_masked_mse)\n",
    "    inverse_masked_denominator = tf.reduce_sum(inverse_mask)\n",
    "    inverse_masked_batch_level_mse = inverse_masked_numerator / inverse_masked_denominator\n",
    "    \n",
    "    \n",
    "    # print debug information\n",
    "    tf.print(\"-----------------------------\")\n",
    "    tf.print(\"Standard batch-level MSE:\", standard_batch_level_mse)\n",
    "    tf.print(\"Standard batch-level MSE using reduce_sum:\", standard_batch_level_mse_reduce_sum)\n",
    "    tf.print(\"Mask length:\", tf.reduce_sum(mask))\n",
    "    tf.print(\"Inverse mask length:\", tf.reduce_sum(inverse_mask))\n",
    "    tf.print(\"n lables in batch:\", len(labels_batch))\n",
    "    tf.print(\" \")\n",
    "    tf.print(\"masked sum:\", tf.reduce_sum(masked_mse))\n",
    "    tf.print(\"inverse masked sum:\", tf.reduce_sum(inverse_masked_mse))\n",
    "    tf.print(\" \")\n",
    "    tf.print(\"masked numerator:\", masked_numerator)\n",
    "    tf.print(\"unmasked numerator:\", inverse_masked_numerator)\n",
    "    tf.print(\"masked denominator:\", masked_denominator)\n",
    "    tf.print(\"unmasked denominator:\", inverse_masked_denominator)\n",
    "    tf.print(\"Masked batch level MSE:\", masked_batch_level_mse)\n",
    "    tf.print(\"Inverse masked batch level MSE:\", inverse_masked_batch_level_mse)\n",
    "    tf.print(\" \")\n",
    "    tf.print(\"point level mse:\", mse)\n",
    "    tf.print(\" \")\n",
    "    tf.print(\"mse shape:\", mse.shape)\n",
    "    tf.print(\"mask shape:\", mask.shape)\n",
    "    tf.print(\"inverse mask shape:\", inverse_mask.shape)\n",
    "    tf.print(\"mask\", mask)\n",
    "    tf.print(\"inverse mask\", inverse_mask)\n",
    "    tf.print(\"masked point level mse:\", masked_mse)\n",
    "    tf.print(\"inverse masked point level mse:\", inverse_masked_mse)\n",
    "    # Return the masked MSE \n",
    "    return masked_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.26178348\n",
      "Standard batch-level MSE using reduce_sum: 1.26178348\n",
      "Mask length: 26\n",
      "Inverse mask length: 6\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 33.388443\n",
      "inverse masked sum: 6.98863125\n",
      " \n",
      "masked numerator: 33.388443\n",
      "unmasked numerator: 6.98863125\n",
      "masked denominator: 26\n",
      "unmasked denominator: 6\n",
      "Masked batch level MSE: 1.28417087\n",
      "Inverse masked batch level MSE: 1.16477191\n",
      " \n",
      "point level mse: [1.05708814 1.12369573 0.571552336 ... 0.399199903 0.922481239 0.632239699]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 0 ... 1 1 1]\n",
      "inverse mask [0 0 1 ... 0 0 0]\n",
      "masked point level mse: [1.05708814 1.12369573 0 ... 0.399199903 0.922481239 0.632239699]\n",
      "inverse masked point level mse: [0 0 0.571552336 ... 0 0 0]\n",
      "Step 0: Loss = [1.0570881  1.1236957  0.         1.5123093  0.         0.6859636\n",
      " 0.         1.0068898  0.5234696  2.4308188  1.5492734  0.39305407\n",
      " 0.8698703  0.         9.675141   0.6130801  0.5739663  0.\n",
      " 1.78475    0.5642363  0.5706226  1.643656   1.8828782  0.5515443\n",
      " 0.36476472 0.         0.4983972  0.31275028 1.2463006  0.3991999\n",
      " 0.92248124 0.6322397 ]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.991343379\n",
      "Standard batch-level MSE using reduce_sum: 0.991343379\n",
      "Mask length: 25\n",
      "Inverse mask length: 7\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 21.3570747\n",
      "inverse masked sum: 10.3659143\n",
      " \n",
      "masked numerator: 21.3570747\n",
      "unmasked numerator: 10.3659143\n",
      "masked denominator: 25\n",
      "unmasked denominator: 7\n",
      "Masked batch level MSE: 0.854283\n",
      "Inverse masked batch level MSE: 1.48084486\n",
      " \n",
      "point level mse: [0.467606515 1.05310714 0.927221715 ... 1.05588341 4.38781452 1.71954548]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 1 0 0]\n",
      "inverse mask [0 0 0 ... 0 1 1]\n",
      "masked point level mse: [0.467606515 1.05310714 0.927221715 ... 1.05588341 0 0]\n",
      "inverse masked point level mse: [0 0 0 ... 0 4.38781452 1.71954548]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.11313963\n",
      "Standard batch-level MSE using reduce_sum: 1.11313963\n",
      "Mask length: 23\n",
      "Inverse mask length: 9\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 18.6944\n",
      "inverse masked sum: 16.9260712\n",
      " \n",
      "masked numerator: 18.6944\n",
      "unmasked numerator: 16.9260712\n",
      "masked denominator: 23\n",
      "unmasked denominator: 9\n",
      "Masked batch level MSE: 0.81280005\n",
      "Inverse masked batch level MSE: 1.8806746\n",
      " \n",
      "point level mse: [0.434130788 1.06043601 0.426983505 ... 0.804679 1.05779064 1.26194179]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 0 1 1]\n",
      "inverse mask [0 0 0 ... 1 0 0]\n",
      "masked point level mse: [0.434130788 1.06043601 0.426983505 ... 0 1.05779064 1.26194179]\n",
      "inverse masked point level mse: [0 0 0 ... 0.804679 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.86148727\n",
      "Standard batch-level MSE using reduce_sum: 1.86148727\n",
      "Mask length: 20\n",
      "Inverse mask length: 12\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 15.837595\n",
      "inverse masked sum: 43.7299957\n",
      " \n",
      "masked numerator: 15.837595\n",
      "unmasked numerator: 43.7299957\n",
      "masked denominator: 20\n",
      "unmasked denominator: 12\n",
      "Masked batch level MSE: 0.791879773\n",
      "Inverse masked batch level MSE: 3.64416623\n",
      " \n",
      "point level mse: [4.62981749 0.606409729 1.96763384 ... 0.287638962 0.582366049 4.00771]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 1 1 0]\n",
      "inverse mask [1 0 0 ... 0 0 1]\n",
      "masked point level mse: [0 0.606409729 1.96763384 ... 0.287638962 0.582366049 0]\n",
      "inverse masked point level mse: [4.62981749 0 0 ... 0 0 4.00771]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.36923099\n",
      "Standard batch-level MSE using reduce_sum: 1.36923099\n",
      "Mask length: 18\n",
      "Inverse mask length: 14\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 18.6258984\n",
      "inverse masked sum: 25.1894951\n",
      " \n",
      "masked numerator: 18.6258984\n",
      "unmasked numerator: 25.1894951\n",
      "masked denominator: 18\n",
      "unmasked denominator: 14\n",
      "Masked batch level MSE: 1.03477216\n",
      "Inverse masked batch level MSE: 1.79924965\n",
      " \n",
      "point level mse: [0.891195655 1.49719012 1.09672153 ... 0.36654675 4.80527067 0.905672073]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 0 ... 1 1 0]\n",
      "inverse mask [1 1 1 ... 0 0 1]\n",
      "masked point level mse: [0 0 0 ... 0.36654675 4.80527067 0]\n",
      "inverse masked point level mse: [0.891195655 1.49719012 1.09672153 ... 0 0 0.905672073]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.07939672\n",
      "Standard batch-level MSE using reduce_sum: 1.07939672\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 19.6395607\n",
      "inverse masked sum: 14.9011326\n",
      " \n",
      "masked numerator: 19.6395607\n",
      "unmasked numerator: 14.9011326\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 0.892707288\n",
      "Inverse masked batch level MSE: 1.49011326\n",
      " \n",
      "point level mse: [1.5101006 0.472954243 1.42061043 ... 2.0979948 0.909055054 0.523103952]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 0 ... 0 1 1]\n",
      "inverse mask [1 0 1 ... 1 0 0]\n",
      "masked point level mse: [0 0.472954243 0 ... 0 0.909055054 0.523103952]\n",
      "inverse masked point level mse: [1.5101006 0 1.42061043 ... 2.0979948 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 13:42:39.446520: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [455,1]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-08-29 13:42:39.446665: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [455,1]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Standard batch-level MSE: 1.33759427\n",
      "Standard batch-level MSE using reduce_sum: 1.33759427\n",
      "Mask length: 18\n",
      "Inverse mask length: 14\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 20.0552483\n",
      "inverse masked sum: 22.7477627\n",
      " \n",
      "masked numerator: 20.0552483\n",
      "unmasked numerator: 22.7477627\n",
      "masked denominator: 18\n",
      "unmasked denominator: 14\n",
      "Masked batch level MSE: 1.11418045\n",
      "Inverse masked batch level MSE: 1.62484014\n",
      " \n",
      "point level mse: [1.13122976 9.40192127 0.54452157 ... 0.408056587 2.36322331 0.48443529]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 1 ... 0 0 0]\n",
      "inverse mask [1 1 0 ... 1 1 1]\n",
      "masked point level mse: [0 0 0.54452157 ... 0 0 0]\n",
      "inverse masked point level mse: [1.13122976 9.40192127 0 ... 0.408056587 2.36322331 0.48443529]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.43153\n",
      "Standard batch-level MSE using reduce_sum: 1.43153\n",
      "Mask length: 21\n",
      "Inverse mask length: 11\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 17.6724281\n",
      "inverse masked sum: 28.1365356\n",
      " \n",
      "masked numerator: 17.6724281\n",
      "unmasked numerator: 28.1365356\n",
      "masked denominator: 21\n",
      "unmasked denominator: 11\n",
      "Masked batch level MSE: 0.841544211\n",
      "Inverse masked batch level MSE: 2.55786681\n",
      " \n",
      "point level mse: [1.44394994 0.357451558 1.23677528 ... 0.413347661 1.67316628 0.490422606]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 1 1 1]\n",
      "inverse mask [1 0 0 ... 0 0 0]\n",
      "masked point level mse: [0 0.357451558 1.23677528 ... 0.413347661 1.67316628 0.490422606]\n",
      "inverse masked point level mse: [1.44394994 0 0 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.28980649\n",
      "Standard batch-level MSE using reduce_sum: 1.28980649\n",
      "Mask length: 18\n",
      "Inverse mask length: 14\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 13.5699692\n",
      "inverse masked sum: 27.7038345\n",
      " \n",
      "masked numerator: 13.5699692\n",
      "unmasked numerator: 27.7038345\n",
      "masked denominator: 18\n",
      "unmasked denominator: 14\n",
      "Masked batch level MSE: 0.753887177\n",
      "Inverse masked batch level MSE: 1.97884536\n",
      " \n",
      "point level mse: [1.70025873 0.46483171 0.382993 ... 0.935185134 3.21548319 0.632122636]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 1 1 1]\n",
      "inverse mask [0 0 0 ... 0 0 0]\n",
      "masked point level mse: [1.70025873 0.46483171 0.382993 ... 0.935185134 3.21548319 0.632122636]\n",
      "inverse masked point level mse: [0 0 0 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.54235792\n",
      "Standard batch-level MSE using reduce_sum: 1.54235792\n",
      "Mask length: 14\n",
      "Inverse mask length: 18\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 8.92960262\n",
      "inverse masked sum: 40.4258499\n",
      " \n",
      "masked numerator: 8.92960262\n",
      "unmasked numerator: 40.4258499\n",
      "masked denominator: 14\n",
      "unmasked denominator: 18\n",
      "Masked batch level MSE: 0.637828767\n",
      "Inverse masked batch level MSE: 2.2458806\n",
      " \n",
      "point level mse: [0.526761413 0.29605943 1.02856433 ... 2.08456326 0.653322041 0.514746428]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 0 ... 0 1 1]\n",
      "inverse mask [1 0 1 ... 1 0 0]\n",
      "masked point level mse: [0 0.29605943 0 ... 0 0.653322041 0.514746428]\n",
      "inverse masked point level mse: [0.526761413 0 1.02856433 ... 2.08456326 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.35905147\n",
      "Standard batch-level MSE using reduce_sum: 1.35905147\n",
      "Mask length: 24\n",
      "Inverse mask length: 8\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 19.1244431\n",
      "inverse masked sum: 24.3652058\n",
      " \n",
      "masked numerator: 19.1244431\n",
      "unmasked numerator: 24.3652058\n",
      "masked denominator: 24\n",
      "unmasked denominator: 8\n",
      "Masked batch level MSE: 0.796851814\n",
      "Inverse masked batch level MSE: 3.04565072\n",
      " \n",
      "point level mse: [4.79311705 0.36498338 0.367169946 ... 1.24245298 3.67245746 2.95471096]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 0 0 1]\n",
      "inverse mask [1 0 0 ... 1 1 0]\n",
      "masked point level mse: [0 0.36498338 0.367169946 ... 0 0 2.95471096]\n",
      "inverse masked point level mse: [4.79311705 0 0 ... 1.24245298 3.67245746 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.26884747\n",
      "Standard batch-level MSE using reduce_sum: 1.26884747\n",
      "Mask length: 16\n",
      "Inverse mask length: 16\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 15.3930693\n",
      "inverse masked sum: 25.2100487\n",
      " \n",
      "masked numerator: 15.3930693\n",
      "unmasked numerator: 25.2100487\n",
      "masked denominator: 16\n",
      "unmasked denominator: 16\n",
      "Masked batch level MSE: 0.962066829\n",
      "Inverse masked batch level MSE: 1.57562804\n",
      " \n",
      "point level mse: [1.03978431 1.33255649 0.534767032 ... 0.57732743 1.62762725 0.332669944]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 1 1 1]\n",
      "inverse mask [0 1 0 ... 0 0 0]\n",
      "masked point level mse: [1.03978431 0 0.534767032 ... 0.57732743 1.62762725 0.332669944]\n",
      "inverse masked point level mse: [0 1.33255649 0 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 2.5636158\n",
      "Standard batch-level MSE using reduce_sum: 2.5636158\n",
      "Mask length: 16\n",
      "Inverse mask length: 16\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 29.0663376\n",
      "inverse masked sum: 52.969368\n",
      " \n",
      "masked numerator: 29.0663376\n",
      "unmasked numerator: 52.969368\n",
      "masked denominator: 16\n",
      "unmasked denominator: 16\n",
      "Masked batch level MSE: 1.8166461\n",
      "Inverse masked batch level MSE: 3.3105855\n",
      " \n",
      "point level mse: [1.08763111 1.01495147 16.8089752 ... 1.15150177 0.299866885 0.811796486]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 1 1 0]\n",
      "inverse mask [1 0 0 ... 0 0 1]\n",
      "masked point level mse: [0 1.01495147 16.8089752 ... 1.15150177 0.299866885 0]\n",
      "inverse masked point level mse: [1.08763111 0 0 ... 0 0 0.811796486]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.19114685\n",
      "Standard batch-level MSE using reduce_sum: 1.19114685\n",
      "Mask length: 21\n",
      "Inverse mask length: 11\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 15.745038\n",
      "inverse masked sum: 22.3716621\n",
      " \n",
      "masked numerator: 15.745038\n",
      "unmasked numerator: 22.3716621\n",
      "masked denominator: 21\n",
      "unmasked denominator: 11\n",
      "Masked batch level MSE: 0.749763727\n",
      "Inverse masked batch level MSE: 2.03378749\n",
      " \n",
      "point level mse: [0.231416389 1.32132351 2.44888139 ... 0.456224144 0.53458482 0.329564035]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 1 1 0]\n",
      "inverse mask [0 0 0 ... 0 0 1]\n",
      "masked point level mse: [0.231416389 1.32132351 2.44888139 ... 0.456224144 0.53458482 0]\n",
      "inverse masked point level mse: [0 0 0 ... 0 0 0.329564035]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 2.10668945\n",
      "Standard batch-level MSE using reduce_sum: 2.10668945\n",
      "Mask length: 4\n",
      "Inverse mask length: 3\n",
      "n lables in batch: 7\n",
      " \n",
      "masked sum: 2.03102899\n",
      "inverse masked sum: 12.7157974\n",
      " \n",
      "masked numerator: 2.03102899\n",
      "unmasked numerator: 12.7157974\n",
      "masked denominator: 4\n",
      "unmasked denominator: 3\n",
      "Masked batch level MSE: 0.507757246\n",
      "Inverse masked batch level MSE: 4.2385993\n",
      " \n",
      "point level mse: [0.616018057 4.53448439 0.469798088 ... 0.153143093 2.79880857 5.38250399]\n",
      " \n",
      "mse shape: TensorShape([7])\n",
      "mask shape: TensorShape([7])\n",
      "inverse mask shape: TensorShape([7])\n",
      "mask [1 0 1 ... 1 0 0]\n",
      "inverse mask [0 1 0 ... 0 1 1]\n",
      "masked point level mse: [0.616018057 0 0.469798088 ... 0.153143093 0 0]\n",
      "inverse masked point level mse: [0 4.53448439 0 ... 0 2.79880857 5.38250399]\n",
      "Epoch 1 completed with final batch loss: [0.61601806 0.         0.4697981  0.7920698  0.1531431  0.\n",
      " 0.        ]\n",
      "Epoch 2/10\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.97819972\n",
      "Standard batch-level MSE using reduce_sum: 0.97819972\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 16.5952187\n",
      "inverse masked sum: 14.7071733\n",
      " \n",
      "masked numerator: 16.5952187\n",
      "unmasked numerator: 14.7071733\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 0.754328132\n",
      "Inverse masked batch level MSE: 1.47071731\n",
      " \n",
      "point level mse: [1.17632377 1.91709244 2.85088038 ... 0.493696064 0.366520494 0.810439706]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 0 ... 1 1 1]\n",
      "inverse mask [0 0 1 ... 0 0 0]\n",
      "masked point level mse: [1.17632377 1.91709244 0 ... 0.493696064 0.366520494 0.810439706]\n",
      "inverse masked point level mse: [0 0 2.85088038 ... 0 0 0]\n",
      "Step 0: Loss = [1.1763238  1.9170924  0.         0.         0.40644494 0.4867578\n",
      " 0.36750272 1.5318512  0.         0.         0.7882855  0.63320994\n",
      " 0.         0.         1.4059931  0.41553625 0.71361387 0.\n",
      " 0.         0.5253237  0.         1.4355675  0.3810666  1.1704942\n",
      " 0.28860873 0.         0.6339207  0.35039225 0.29657677 0.49369606\n",
      " 0.3665205  0.8104397 ]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.41094422\n",
      "Standard batch-level MSE using reduce_sum: 1.41094422\n",
      "Mask length: 21\n",
      "Inverse mask length: 11\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 30.1144867\n",
      "inverse masked sum: 15.0357323\n",
      " \n",
      "masked numerator: 30.1144867\n",
      "unmasked numerator: 15.0357323\n",
      "masked denominator: 21\n",
      "unmasked denominator: 11\n",
      "Masked batch level MSE: 1.43402314\n",
      "Inverse masked batch level MSE: 1.36688471\n",
      " \n",
      "point level mse: [0.447965473 1.09134448 2.33004785 ... 0.536877692 1.36044383 2.02857161]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 0 ... 0 1 1]\n",
      "inverse mask [0 0 1 ... 1 0 0]\n",
      "masked point level mse: [0.447965473 1.09134448 0 ... 0 1.36044383 2.02857161]\n",
      "inverse masked point level mse: [0 0 2.33004785 ... 0.536877692 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.35729468\n",
      "Standard batch-level MSE using reduce_sum: 1.35729468\n",
      "Mask length: 21\n",
      "Inverse mask length: 11\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 16.7801628\n",
      "inverse masked sum: 26.6532669\n",
      " \n",
      "masked numerator: 16.7801628\n",
      "unmasked numerator: 26.6532669\n",
      "masked denominator: 21\n",
      "unmasked denominator: 11\n",
      "Masked batch level MSE: 0.799055398\n",
      "Inverse masked batch level MSE: 2.42302418\n",
      " \n",
      "point level mse: [0.494145274 0.510521472 1.01112962 ... 0.751333356 0.483884 1.0302124]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 1 1 1]\n",
      "inverse mask [0 1 0 ... 0 0 0]\n",
      "masked point level mse: [0.494145274 0 1.01112962 ... 0.751333356 0.483884 1.0302124]\n",
      "inverse masked point level mse: [0 0.510521472 0 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.953235805\n",
      "Standard batch-level MSE using reduce_sum: 0.953235805\n",
      "Mask length: 19\n",
      "Inverse mask length: 13\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 14.4572868\n",
      "inverse masked sum: 16.0462589\n",
      " \n",
      "masked numerator: 14.4572868\n",
      "unmasked numerator: 16.0462589\n",
      "masked denominator: 19\n",
      "unmasked denominator: 13\n",
      "Masked batch level MSE: 0.760909855\n",
      "Inverse masked batch level MSE: 1.23432755\n",
      " \n",
      "point level mse: [0.805277228 0.900472522 0.882515609 ... 1.42638242 0.277967542 3.11388898]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 0 0 1]\n",
      "inverse mask [0 1 0 ... 1 1 0]\n",
      "masked point level mse: [0.805277228 0 0.882515609 ... 0 0 3.11388898]\n",
      "inverse masked point level mse: [0 0.900472522 0 ... 1.42638242 0.277967542 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.47252357\n",
      "Standard batch-level MSE using reduce_sum: 1.47252357\n",
      "Mask length: 19\n",
      "Inverse mask length: 13\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 16.4054565\n",
      "inverse masked sum: 30.7152958\n",
      " \n",
      "masked numerator: 16.4054565\n",
      "unmasked numerator: 30.7152958\n",
      "masked denominator: 19\n",
      "unmasked denominator: 13\n",
      "Masked batch level MSE: 0.863445103\n",
      "Inverse masked batch level MSE: 2.36271501\n",
      " \n",
      "point level mse: [0.394677252 0.361240596 2.84455323 ... 0.786043704 0.474322945 0.696475267]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 0 ... 1 1 1]\n",
      "inverse mask [1 1 1 ... 0 0 0]\n",
      "masked point level mse: [0 0 0 ... 0.786043704 0.474322945 0.696475267]\n",
      "inverse masked point level mse: [0.394677252 0.361240596 2.84455323 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 2.05165291\n",
      "Standard batch-level MSE using reduce_sum: 2.05165291\n",
      "Mask length: 19\n",
      "Inverse mask length: 13\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 12.6468239\n",
      "inverse masked sum: 53.0060692\n",
      " \n",
      "masked numerator: 12.6468239\n",
      "unmasked numerator: 53.0060692\n",
      "masked denominator: 19\n",
      "unmasked denominator: 13\n",
      "Masked batch level MSE: 0.665622294\n",
      "Inverse masked batch level MSE: 4.07738972\n",
      " \n",
      "point level mse: [1.46452606 7.06663227 0.435736924 ... 4.97119427 0.518225789 4.40335321]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 1 ... 0 1 0]\n",
      "inverse mask [1 1 0 ... 1 0 1]\n",
      "masked point level mse: [0 0 0.435736924 ... 0 0.518225789 0]\n",
      "inverse masked point level mse: [1.46452606 7.06663227 0 ... 4.97119427 0 4.40335321]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.912460625\n",
      "Standard batch-level MSE using reduce_sum: 0.912460625\n",
      "Mask length: 20\n",
      "Inverse mask length: 12\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 14.1345119\n",
      "inverse masked sum: 15.0642281\n",
      " \n",
      "masked numerator: 14.1345119\n",
      "unmasked numerator: 15.0642281\n",
      "masked denominator: 20\n",
      "unmasked denominator: 12\n",
      "Masked batch level MSE: 0.706725597\n",
      "Inverse masked batch level MSE: 1.25535238\n",
      " \n",
      "point level mse: [2.07871699 1.37083662 0.572636843 ... 0.421446741 0.820452929 1.06343043]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 0 ... 1 1 1]\n",
      "inverse mask [0 1 1 ... 0 0 0]\n",
      "masked point level mse: [2.07871699 0 0 ... 0.421446741 0.820452929 1.06343043]\n",
      "inverse masked point level mse: [0 1.37083662 0.572636843 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.50384355\n",
      "Standard batch-level MSE using reduce_sum: 1.50384355\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 16.3494644\n",
      "inverse masked sum: 31.773531\n",
      " \n",
      "masked numerator: 16.3494644\n",
      "unmasked numerator: 31.773531\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 0.743157446\n",
      "Inverse masked batch level MSE: 3.17735314\n",
      " \n",
      "point level mse: [0.556197405 0.231014624 0.363745 ... 0.940191507 0.901885092 0.386884689]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 1 1 1]\n",
      "inverse mask [0 1 0 ... 0 0 0]\n",
      "masked point level mse: [0.556197405 0 0.363745 ... 0.940191507 0.901885092 0.386884689]\n",
      "inverse masked point level mse: [0 0.231014624 0 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 2.07609987\n",
      "Standard batch-level MSE using reduce_sum: 2.07609987\n",
      "Mask length: 17\n",
      "Inverse mask length: 15\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 10.5551414\n",
      "inverse masked sum: 55.8800507\n",
      " \n",
      "masked numerator: 10.5551414\n",
      "unmasked numerator: 55.8800507\n",
      "masked denominator: 17\n",
      "unmasked denominator: 15\n",
      "Masked batch level MSE: 0.620890677\n",
      "Inverse masked batch level MSE: 3.72533679\n",
      " \n",
      "point level mse: [0.641657233 0.290558487 0.658081651 ... 0.72350347 3.46346927 1.43623269]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 1 0 0]\n",
      "inverse mask [0 0 0 ... 0 1 1]\n",
      "masked point level mse: [0.641657233 0.290558487 0.658081651 ... 0.72350347 0 0]\n",
      "inverse masked point level mse: [0 0 0 ... 0 3.46346927 1.43623269]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.83084393\n",
      "Standard batch-level MSE using reduce_sum: 1.83084393\n",
      "Mask length: 19\n",
      "Inverse mask length: 13\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 33.7592545\n",
      "inverse masked sum: 24.827755\n",
      " \n",
      "masked numerator: 33.7592545\n",
      "unmasked numerator: 24.827755\n",
      "masked denominator: 19\n",
      "unmasked denominator: 13\n",
      "Masked batch level MSE: 1.7768029\n",
      "Inverse masked batch level MSE: 1.90982735\n",
      " \n",
      "point level mse: [0.567754209 0.68787 0.973379314 ... 0.270849228 1.93534327 1.9157064]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 1 0 0]\n",
      "inverse mask [0 1 0 ... 0 1 1]\n",
      "masked point level mse: [0.567754209 0 0.973379314 ... 0.270849228 0 0]\n",
      "inverse masked point level mse: [0 0.68787 0 ... 0 1.93534327 1.9157064]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.910311401\n",
      "Standard batch-level MSE using reduce_sum: 0.910311401\n",
      "Mask length: 21\n",
      "Inverse mask length: 11\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 9.78729153\n",
      "inverse masked sum: 19.3426704\n",
      " \n",
      "masked numerator: 9.78729153\n",
      "unmasked numerator: 19.3426704\n",
      "masked denominator: 21\n",
      "unmasked denominator: 11\n",
      "Masked batch level MSE: 0.466061503\n",
      "Inverse masked batch level MSE: 1.75842464\n",
      " \n",
      "point level mse: [0.977665663 1.02857757 0.197302684 ... 0.326512635 0.67831707 0.139638141]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 1 ... 0 1 1]\n",
      "inverse mask [1 1 0 ... 1 0 0]\n",
      "masked point level mse: [0 0 0.197302684 ... 0 0.67831707 0.139638141]\n",
      "inverse masked point level mse: [0.977665663 1.02857757 0 ... 0.326512635 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.960678\n",
      "Standard batch-level MSE using reduce_sum: 0.960678\n",
      "Mask length: 24\n",
      "Inverse mask length: 8\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 14.3198929\n",
      "inverse masked sum: 16.4218025\n",
      " \n",
      "masked numerator: 14.3198929\n",
      "unmasked numerator: 16.4218025\n",
      "masked denominator: 24\n",
      "unmasked denominator: 8\n",
      "Masked batch level MSE: 0.596662223\n",
      "Inverse masked batch level MSE: 2.05272532\n",
      " \n",
      "point level mse: [1.29321826 0.614542425 1.67264628 ... 0.659989417 0.609283149 0.708563149]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 0 ... 1 1 1]\n",
      "inverse mask [1 0 1 ... 0 0 0]\n",
      "masked point level mse: [0 0.614542425 0 ... 0.659989417 0.609283149 0.708563149]\n",
      "inverse masked point level mse: [1.29321826 0 1.67264628 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.22379541\n",
      "Standard batch-level MSE using reduce_sum: 1.22379541\n",
      "Mask length: 17\n",
      "Inverse mask length: 15\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 9.93258572\n",
      "inverse masked sum: 29.2288685\n",
      " \n",
      "masked numerator: 9.93258572\n",
      "unmasked numerator: 29.2288685\n",
      "masked denominator: 17\n",
      "unmasked denominator: 15\n",
      "Masked batch level MSE: 0.584269762\n",
      "Inverse masked batch level MSE: 1.94859123\n",
      " \n",
      "point level mse: [6.24634504 0.444795728 2.62221932 ... 0.338154644 0.573978364 0.703338504]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 0 ... 1 0 0]\n",
      "inverse mask [1 0 1 ... 0 1 1]\n",
      "masked point level mse: [0 0.444795728 0 ... 0.338154644 0 0]\n",
      "inverse masked point level mse: [6.24634504 0 2.62221932 ... 0 0.573978364 0.703338504]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.10249281\n",
      "Standard batch-level MSE using reduce_sum: 1.10249281\n",
      "Mask length: 20\n",
      "Inverse mask length: 12\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 15.1859188\n",
      "inverse masked sum: 20.093853\n",
      " \n",
      "masked numerator: 15.1859188\n",
      "unmasked numerator: 20.093853\n",
      "masked denominator: 20\n",
      "unmasked denominator: 12\n",
      "Masked batch level MSE: 0.75929594\n",
      "Inverse masked batch level MSE: 1.67448771\n",
      " \n",
      "point level mse: [1.08308089 0.247675627 1.19364202 ... 0.261455625 0.259104 1.15393019]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 0 ... 1 1 1]\n",
      "inverse mask [0 0 1 ... 0 0 0]\n",
      "masked point level mse: [1.08308089 0.247675627 0 ... 0.261455625 0.259104 1.15393019]\n",
      "inverse masked point level mse: [0 0 1.19364202 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.493602902\n",
      "Standard batch-level MSE using reduce_sum: 0.493602902\n",
      "Mask length: 5\n",
      "Inverse mask length: 2\n",
      "n lables in batch: 7\n",
      " \n",
      "masked sum: 2.00989699\n",
      "inverse masked sum: 1.44532323\n",
      " \n",
      "masked numerator: 2.00989699\n",
      "unmasked numerator: 1.44532323\n",
      "masked denominator: 5\n",
      "unmasked denominator: 2\n",
      "Masked batch level MSE: 0.401979387\n",
      "Inverse masked batch level MSE: 0.722661614\n",
      " \n",
      "point level mse: [0.767898619 0.613987 0.532814324 ... 0.159787 0.196681842 0.27154249]\n",
      " \n",
      "mse shape: TensorShape([7])\n",
      "mask shape: TensorShape([7])\n",
      "inverse mask shape: TensorShape([7])\n",
      "mask [1 1 0 ... 1 1 1]\n",
      "inverse mask [0 0 1 ... 0 0 0]\n",
      "masked point level mse: [0.767898619 0.613987 0 ... 0.159787 0.196681842 0.27154249]\n",
      "inverse masked point level mse: [0 0 0.532814324 ... 0 0 0]\n",
      "Epoch 2 completed with final batch loss: [0.7678986  0.613987   0.         0.         0.159787   0.19668184\n",
      " 0.2715425 ]\n",
      "Epoch 3/10\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 2.06526542\n",
      "Standard batch-level MSE using reduce_sum: 2.06526542\n",
      "Mask length: 18\n",
      "Inverse mask length: 14\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 27.9139\n",
      "inverse masked sum: 38.1745911\n",
      " \n",
      "masked numerator: 27.9139\n",
      "unmasked numerator: 38.1745911\n",
      "masked denominator: 18\n",
      "unmasked denominator: 14\n",
      "Masked batch level MSE: 1.55077219\n",
      "Inverse masked batch level MSE: 2.72675657\n",
      " \n",
      "point level mse: [0.304220974 0.784512103 0.177703559 ... 0.280511558 1.41888034 11.8662424]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 0 1 0]\n",
      "inverse mask [0 0 0 ... 1 0 1]\n",
      "masked point level mse: [0.304220974 0.784512103 0.177703559 ... 0 1.41888034 0]\n",
      "inverse masked point level mse: [0 0 0 ... 0.280511558 0 11.8662424]\n",
      "Step 0: Loss = [ 0.30422097  0.7845121   0.17770356  0.7437995   3.6694968   0.\n",
      "  0.          0.          0.36449936  0.          0.          0.455517\n",
      "  0.7446957   0.          1.305613   14.699444    0.          0.\n",
      "  0.602444    0.44071522  0.          0.27999777  0.          0.34878808\n",
      "  0.6604754   0.          0.57648575  0.33661205  0.          0.\n",
      "  1.4188803   0.        ]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.979400575\n",
      "Standard batch-level MSE using reduce_sum: 0.979400575\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 15.0533323\n",
      "inverse masked sum: 16.287487\n",
      " \n",
      "masked numerator: 15.0533323\n",
      "unmasked numerator: 16.287487\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 0.684242368\n",
      "Inverse masked batch level MSE: 1.62874866\n",
      " \n",
      "point level mse: [0.813316941 0.49152258 0.644043922 ... 1.26346064 0.978944123 1.05070269]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 0 ... 1 1 0]\n",
      "inverse mask [0 0 1 ... 0 0 1]\n",
      "masked point level mse: [0.813316941 0.49152258 0 ... 1.26346064 0.978944123 0]\n",
      "inverse masked point level mse: [0 0 0.644043922 ... 0 0 1.05070269]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.42276728\n",
      "Standard batch-level MSE using reduce_sum: 1.42276728\n",
      "Mask length: 20\n",
      "Inverse mask length: 12\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 15.9423122\n",
      "inverse masked sum: 29.5862427\n",
      " \n",
      "masked numerator: 15.9423122\n",
      "unmasked numerator: 29.5862427\n",
      "masked denominator: 20\n",
      "unmasked denominator: 12\n",
      "Masked batch level MSE: 0.797115624\n",
      "Inverse masked batch level MSE: 2.46552014\n",
      " \n",
      "point level mse: [0.650918067 2.40439 1.18012345 ... 3.68826532 1.82285166 0.0840243176]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 0 ... 0 1 1]\n",
      "inverse mask [0 1 1 ... 1 0 0]\n",
      "masked point level mse: [0.650918067 0 0 ... 0 1.82285166 0.0840243176]\n",
      "inverse masked point level mse: [0 2.40439 1.18012345 ... 3.68826532 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.872533679\n",
      "Standard batch-level MSE using reduce_sum: 0.872533679\n",
      "Mask length: 21\n",
      "Inverse mask length: 11\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 15.7833767\n",
      "inverse masked sum: 12.1377029\n",
      " \n",
      "masked numerator: 15.7833767\n",
      "unmasked numerator: 12.1377029\n",
      "masked denominator: 21\n",
      "unmasked denominator: 11\n",
      "Masked batch level MSE: 0.751589358\n",
      "Inverse masked batch level MSE: 1.10342753\n",
      " \n",
      "point level mse: [0.158129081 5.02823067 0.585727692 ... 0.261548072 0.381218612 0.260190099]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 0 ... 1 1 0]\n",
      "inverse mask [0 1 1 ... 0 0 1]\n",
      "masked point level mse: [0.158129081 0 0 ... 0.261548072 0.381218612 0]\n",
      "inverse masked point level mse: [0 5.02823067 0.585727692 ... 0 0 0.260190099]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.09032035\n",
      "Standard batch-level MSE using reduce_sum: 1.09032035\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 13.2194958\n",
      "inverse masked sum: 21.6707592\n",
      " \n",
      "masked numerator: 13.2194958\n",
      "unmasked numerator: 21.6707592\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 0.600886166\n",
      "Inverse masked batch level MSE: 2.16707587\n",
      " \n",
      "point level mse: [0.591614604 1.80216229 0.266996861 ... 0.337931097 1.10458052 1.4451046]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 1 0 0]\n",
      "inverse mask [0 1 0 ... 0 1 1]\n",
      "masked point level mse: [0.591614604 0 0.266996861 ... 0.337931097 0 0]\n",
      "inverse masked point level mse: [0 1.80216229 0 ... 0 1.10458052 1.4451046]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.43820107\n",
      "Standard batch-level MSE using reduce_sum: 1.43820107\n",
      "Mask length: 15\n",
      "Inverse mask length: 17\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 9.85906887\n",
      "inverse masked sum: 36.1633606\n",
      " \n",
      "masked numerator: 9.85906887\n",
      "unmasked numerator: 36.1633606\n",
      "masked denominator: 15\n",
      "unmasked denominator: 17\n",
      "Masked batch level MSE: 0.657271266\n",
      "Inverse masked batch level MSE: 2.12725639\n",
      " \n",
      "point level mse: [0.224796072 0.583383858 0.68011868 ... 2.12495923 0.550995409 0.335839957]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 0 1 1]\n",
      "inverse mask [0 1 0 ... 1 0 0]\n",
      "masked point level mse: [0.224796072 0 0.68011868 ... 0 0.550995409 0.335839957]\n",
      "inverse masked point level mse: [0 0.583383858 0 ... 2.12495923 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 2.15086198\n",
      "Standard batch-level MSE using reduce_sum: 2.15086198\n",
      "Mask length: 18\n",
      "Inverse mask length: 14\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 9.01615715\n",
      "inverse masked sum: 59.8114281\n",
      " \n",
      "masked numerator: 9.01615715\n",
      "unmasked numerator: 59.8114281\n",
      "masked denominator: 18\n",
      "unmasked denominator: 14\n",
      "Masked batch level MSE: 0.500897646\n",
      "Inverse masked batch level MSE: 4.27224493\n",
      " \n",
      "point level mse: [1.76410675 0.141107947 1.44598925 ... 0.19613634 0.327768385 2.20302153]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 1 1 0]\n",
      "inverse mask [1 0 0 ... 0 0 1]\n",
      "masked point level mse: [0 0.141107947 1.44598925 ... 0.19613634 0.327768385 0]\n",
      "inverse masked point level mse: [1.76410675 0 0 ... 0 0 2.20302153]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.55714214\n",
      "Standard batch-level MSE using reduce_sum: 1.55714214\n",
      "Mask length: 21\n",
      "Inverse mask length: 11\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 13.5592651\n",
      "inverse masked sum: 36.2692833\n",
      " \n",
      "masked numerator: 13.5592651\n",
      "unmasked numerator: 36.2692833\n",
      "masked denominator: 21\n",
      "unmasked denominator: 11\n",
      "Masked batch level MSE: 0.645679295\n",
      "Inverse masked batch level MSE: 3.29720759\n",
      " \n",
      "point level mse: [0.446119577 21.6853676 0.498953193 ... 0.427921355 0.395202607 1.1982677]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 1 1 1]\n",
      "inverse mask [0 1 0 ... 0 0 0]\n",
      "masked point level mse: [0.446119577 0 0.498953193 ... 0.427921355 0.395202607 1.1982677]\n",
      "inverse masked point level mse: [0 21.6853676 0 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.992992759\n",
      "Standard batch-level MSE using reduce_sum: 0.992992759\n",
      "Mask length: 21\n",
      "Inverse mask length: 11\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 19.173439\n",
      "inverse masked sum: 12.6023293\n",
      " \n",
      "masked numerator: 19.173439\n",
      "unmasked numerator: 12.6023293\n",
      "masked denominator: 21\n",
      "unmasked denominator: 11\n",
      "Masked batch level MSE: 0.913020909\n",
      "Inverse masked batch level MSE: 1.14566624\n",
      " \n",
      "point level mse: [0.487757593 0.733757555 1.51286721 ... 0.484608442 2.43661761 0.953508914]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 0 ... 1 0 1]\n",
      "inverse mask [0 0 1 ... 0 1 0]\n",
      "masked point level mse: [0.487757593 0.733757555 0 ... 0.484608442 0 0.953508914]\n",
      "inverse masked point level mse: [0 0 1.51286721 ... 0 2.43661761 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.2687248\n",
      "Standard batch-level MSE using reduce_sum: 1.2687248\n",
      "Mask length: 23\n",
      "Inverse mask length: 9\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 17.3674717\n",
      "inverse masked sum: 23.2317238\n",
      " \n",
      "masked numerator: 17.3674717\n",
      "unmasked numerator: 23.2317238\n",
      "masked denominator: 23\n",
      "unmasked denominator: 9\n",
      "Masked batch level MSE: 0.755107462\n",
      "Inverse masked batch level MSE: 2.58130264\n",
      " \n",
      "point level mse: [0.612417579 0.577077568 2.41465926 ... 0.306269199 1.4601593 0.331480861]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 0 ... 1 0 1]\n",
      "inverse mask [0 0 1 ... 0 1 0]\n",
      "masked point level mse: [0.612417579 0.577077568 0 ... 0.306269199 0 0.331480861]\n",
      "inverse masked point level mse: [0 0 2.41465926 ... 0 1.4601593 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.03228486\n",
      "Standard batch-level MSE using reduce_sum: 1.03228486\n",
      "Mask length: 21\n",
      "Inverse mask length: 11\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 16.8992119\n",
      "inverse masked sum: 16.1339054\n",
      " \n",
      "masked numerator: 16.8992119\n",
      "unmasked numerator: 16.1339054\n",
      "masked denominator: 21\n",
      "unmasked denominator: 11\n",
      "Masked batch level MSE: 0.804724395\n",
      "Inverse masked batch level MSE: 1.46671867\n",
      " \n",
      "point level mse: [1.20712638 0.32544449 0.535638273 ... 6.74127388 0.880308867 1.01151443]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 0 ... 1 1 1]\n",
      "inverse mask [1 0 1 ... 0 0 0]\n",
      "masked point level mse: [0 0.32544449 0 ... 6.74127388 0.880308867 1.01151443]\n",
      "inverse masked point level mse: [1.20712638 0 0.535638273 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.999798834\n",
      "Standard batch-level MSE using reduce_sum: 0.999798834\n",
      "Mask length: 20\n",
      "Inverse mask length: 12\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 10.8996696\n",
      "inverse masked sum: 21.0938931\n",
      " \n",
      "masked numerator: 10.8996696\n",
      "unmasked numerator: 21.0938931\n",
      "masked denominator: 20\n",
      "unmasked denominator: 12\n",
      "Masked batch level MSE: 0.544983506\n",
      "Inverse masked batch level MSE: 1.75782442\n",
      " \n",
      "point level mse: [1.52504623 0.988280058 1.13969159 ... 0.482032686 0.347369462 0.420767963]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 0 ... 1 0 1]\n",
      "inverse mask [1 1 1 ... 0 1 0]\n",
      "masked point level mse: [0 0 0 ... 0.482032686 0 0.420767963]\n",
      "inverse masked point level mse: [1.52504623 0.988280058 1.13969159 ... 0 0.347369462 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.90458715\n",
      "Standard batch-level MSE using reduce_sum: 0.90458715\n",
      "Mask length: 18\n",
      "Inverse mask length: 14\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 10.85742\n",
      "inverse masked sum: 18.0893688\n",
      " \n",
      "masked numerator: 10.85742\n",
      "unmasked numerator: 18.0893688\n",
      "masked denominator: 18\n",
      "unmasked denominator: 14\n",
      "Masked batch level MSE: 0.60319\n",
      "Inverse masked batch level MSE: 1.29209781\n",
      " \n",
      "point level mse: [0.329251021 1.45899069 1.48493707 ... 0.403300613 0.566595376 1.25009859]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 0 ... 1 1 0]\n",
      "inverse mask [0 1 1 ... 0 0 1]\n",
      "masked point level mse: [0.329251021 0 0 ... 0.403300613 0.566595376 0]\n",
      "inverse masked point level mse: [0 1.45899069 1.48493707 ... 0 0 1.25009859]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.40307331\n",
      "Standard batch-level MSE using reduce_sum: 1.40307331\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 16.0522308\n",
      "inverse masked sum: 28.8461151\n",
      " \n",
      "masked numerator: 16.0522308\n",
      "unmasked numerator: 28.8461151\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 0.729646862\n",
      "Inverse masked batch level MSE: 2.88461161\n",
      " \n",
      "point level mse: [5.41547441 5.67906427 0.425745279 ... 0.486068159 0.371020526 4.28261471]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 1 ... 1 1 0]\n",
      "inverse mask [1 1 0 ... 0 0 1]\n",
      "masked point level mse: [0 0 0.425745279 ... 0.486068159 0.371020526 0]\n",
      "inverse masked point level mse: [5.41547441 5.67906427 0 ... 0 0 4.28261471]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.675431788\n",
      "Standard batch-level MSE using reduce_sum: 0.675431788\n",
      "Mask length: 4\n",
      "Inverse mask length: 3\n",
      "n lables in batch: 7\n",
      " \n",
      "masked sum: 2.77498364\n",
      "inverse masked sum: 1.95303869\n",
      " \n",
      "masked numerator: 2.77498364\n",
      "unmasked numerator: 1.95303869\n",
      "masked denominator: 4\n",
      "unmasked denominator: 3\n",
      "Masked batch level MSE: 0.693745911\n",
      "Inverse masked batch level MSE: 0.651012897\n",
      " \n",
      "point level mse: [0.471187502 0.601436675 1.36811757 ... 0.393293202 0.334241867 1.15676928]\n",
      " \n",
      "mse shape: TensorShape([7])\n",
      "mask shape: TensorShape([7])\n",
      "inverse mask shape: TensorShape([7])\n",
      "mask [1 1 1 ... 0 1 0]\n",
      "inverse mask [0 0 0 ... 1 0 1]\n",
      "masked point level mse: [0.471187502 0.601436675 1.36811757 ... 0 0.334241867 0]\n",
      "inverse masked point level mse: [0 0 0 ... 0.393293202 0 1.15676928]\n",
      "Epoch 3 completed with final batch loss: [0.4711875  0.6014367  1.3681176  0.         0.         0.33424187\n",
      " 0.        ]\n",
      "Epoch 4/10\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.08699071\n",
      "Standard batch-level MSE using reduce_sum: 1.08699071\n",
      "Mask length: 23\n",
      "Inverse mask length: 9\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 16.4583263\n",
      "inverse masked sum: 18.3253784\n",
      " \n",
      "masked numerator: 16.4583263\n",
      "unmasked numerator: 18.3253784\n",
      "masked denominator: 23\n",
      "unmasked denominator: 9\n",
      "Masked batch level MSE: 0.715579391\n",
      "Inverse masked batch level MSE: 2.03615308\n",
      " \n",
      "point level mse: [0.576862 0.802030802 0.627650082 ... 0.55159 0.917978 2.16222644]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 1 1 0]\n",
      "inverse mask [0 0 0 ... 0 0 1]\n",
      "masked point level mse: [0.576862 0.802030802 0.627650082 ... 0.55159 0.917978 0]\n",
      "inverse masked point level mse: [0 0 0 ... 0 0 2.16222644]\n",
      "Step 0: Loss = [0.576862   0.8020308  0.6276501  0.         0.4837335  0.37471354\n",
      " 0.44437683 0.39889404 0.23979619 1.3355479  0.23583867 0.\n",
      " 0.         0.2236889  2.9750254  0.         0.         0.91932297\n",
      " 0.5408194  0.3773372  0.         0.         0.46756807 0.99272174\n",
      " 0.15547727 0.         1.462737   0.43512088 0.9194953  0.55159\n",
      " 0.917978   0.        ]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.70664918\n",
      "Standard batch-level MSE using reduce_sum: 1.70664918\n",
      "Mask length: 20\n",
      "Inverse mask length: 12\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 8.12336731\n",
      "inverse masked sum: 46.4894066\n",
      " \n",
      "masked numerator: 8.12336731\n",
      "unmasked numerator: 46.4894066\n",
      "masked denominator: 20\n",
      "unmasked denominator: 12\n",
      "Masked batch level MSE: 0.406168371\n",
      "Inverse masked batch level MSE: 3.87411714\n",
      " \n",
      "point level mse: [1.33629644 0.164791197 1.78447282 ... 0.323211044 0.269291341 3.05462646]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 0 ... 1 1 0]\n",
      "inverse mask [1 0 1 ... 0 0 1]\n",
      "masked point level mse: [0 0.164791197 0 ... 0.323211044 0.269291341 0]\n",
      "inverse masked point level mse: [1.33629644 0 1.78447282 ... 0 0 3.05462646]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.43092585\n",
      "Standard batch-level MSE using reduce_sum: 1.43092585\n",
      "Mask length: 15\n",
      "Inverse mask length: 17\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 7.59249115\n",
      "inverse masked sum: 38.1971397\n",
      " \n",
      "masked numerator: 7.59249115\n",
      "unmasked numerator: 38.1971397\n",
      "masked denominator: 15\n",
      "unmasked denominator: 17\n",
      "Masked batch level MSE: 0.506166101\n",
      "Inverse masked batch level MSE: 2.24689054\n",
      " \n",
      "point level mse: [0.918298483 0.332190067 0.333788186 ... 1.70202327 0.43822062 0.366704434]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 0 ... 0 1 1]\n",
      "inverse mask [1 0 1 ... 1 0 0]\n",
      "masked point level mse: [0 0.332190067 0 ... 0 0.43822062 0.366704434]\n",
      "inverse masked point level mse: [0.918298483 0 0.333788186 ... 1.70202327 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.20081282\n",
      "Standard batch-level MSE using reduce_sum: 1.20081282\n",
      "Mask length: 24\n",
      "Inverse mask length: 8\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 27.7261467\n",
      "inverse masked sum: 10.6998634\n",
      " \n",
      "masked numerator: 27.7261467\n",
      "unmasked numerator: 10.6998634\n",
      "masked denominator: 24\n",
      "unmasked denominator: 8\n",
      "Masked batch level MSE: 1.15525615\n",
      "Inverse masked batch level MSE: 1.33748293\n",
      " \n",
      "point level mse: [0.228260651 0.409543872 0.539701581 ... 0.44161588 0.881348073 1.70346832]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 0 ... 1 1 0]\n",
      "inverse mask [0 0 1 ... 0 0 1]\n",
      "masked point level mse: [0.228260651 0.409543872 0 ... 0.44161588 0.881348073 0]\n",
      "inverse masked point level mse: [0 0 0.539701581 ... 0 0 1.70346832]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.24479961\n",
      "Standard batch-level MSE using reduce_sum: 1.24479961\n",
      "Mask length: 19\n",
      "Inverse mask length: 13\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 13.4976768\n",
      "inverse masked sum: 26.3359108\n",
      " \n",
      "masked numerator: 13.4976768\n",
      "unmasked numerator: 26.3359108\n",
      "masked denominator: 19\n",
      "unmasked denominator: 13\n",
      "Masked batch level MSE: 0.710404038\n",
      "Inverse masked batch level MSE: 2.02583933\n",
      " \n",
      "point level mse: [0.198407069 0.363304108 1.84341967 ... 5.40691137 0.969130874 0.471789509]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 0 ... 0 1 0]\n",
      "inverse mask [1 0 1 ... 1 0 1]\n",
      "masked point level mse: [0 0.363304108 0 ... 0 0.969130874 0]\n",
      "inverse masked point level mse: [0.198407069 0 1.84341967 ... 5.40691137 0 0.471789509]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.25148201\n",
      "Standard batch-level MSE using reduce_sum: 1.25148201\n",
      "Mask length: 20\n",
      "Inverse mask length: 12\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 13.749671\n",
      "inverse masked sum: 26.2977524\n",
      " \n",
      "masked numerator: 13.749671\n",
      "unmasked numerator: 26.2977524\n",
      "masked denominator: 20\n",
      "unmasked denominator: 12\n",
      "Masked batch level MSE: 0.687483549\n",
      "Inverse masked batch level MSE: 2.19147944\n",
      " \n",
      "point level mse: [2.66196132 0.506212831 5.49809742 ... 0.970602 0.5096789 0.42561546]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 0 ... 1 1 1]\n",
      "inverse mask [1 1 1 ... 0 0 0]\n",
      "masked point level mse: [0 0 0 ... 0.970602 0.5096789 0.42561546]\n",
      "inverse masked point level mse: [2.66196132 0.506212831 5.49809742 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.32076526\n",
      "Standard batch-level MSE using reduce_sum: 1.32076526\n",
      "Mask length: 21\n",
      "Inverse mask length: 11\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 14.1492939\n",
      "inverse masked sum: 28.1151943\n",
      " \n",
      "masked numerator: 14.1492939\n",
      "unmasked numerator: 28.1151943\n",
      "masked denominator: 21\n",
      "unmasked denominator: 11\n",
      "Masked batch level MSE: 0.673775911\n",
      "Inverse masked batch level MSE: 2.5559268\n",
      " \n",
      "point level mse: [0.248074338 0.647842288 4.9338851 ... 2.23175263 0.922823071 0.255825818]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 0 ... 0 1 1]\n",
      "inverse mask [0 0 1 ... 1 0 0]\n",
      "masked point level mse: [0.248074338 0.647842288 0 ... 0 0.922823071 0.255825818]\n",
      "inverse masked point level mse: [0 0 4.9338851 ... 2.23175263 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.06235647\n",
      "Standard batch-level MSE using reduce_sum: 1.06235647\n",
      "Mask length: 19\n",
      "Inverse mask length: 13\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 9.02191544\n",
      "inverse masked sum: 24.9734955\n",
      " \n",
      "masked numerator: 9.02191544\n",
      "unmasked numerator: 24.9734955\n",
      "masked denominator: 19\n",
      "unmasked denominator: 13\n",
      "Masked batch level MSE: 0.474837661\n",
      "Inverse masked batch level MSE: 1.92103815\n",
      " \n",
      "point level mse: [4.50827646 0.265388 0.195878014 ... 0.798761368 0.561693549 0.984170556]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 1 1 1]\n",
      "inverse mask [1 0 0 ... 0 0 0]\n",
      "masked point level mse: [0 0.265388 0.195878014 ... 0.798761368 0.561693549 0.984170556]\n",
      "inverse masked point level mse: [4.50827646 0 0 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 2.10588765\n",
      "Standard batch-level MSE using reduce_sum: 2.10588765\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 15.9774742\n",
      "inverse masked sum: 51.4109268\n",
      " \n",
      "masked numerator: 15.9774742\n",
      "unmasked numerator: 51.4109268\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 0.726248801\n",
      "Inverse masked batch level MSE: 5.14109278\n",
      " \n",
      "point level mse: [1.07985187 1.27753019 1.45847642 ... 1.70080888 0.691841543 0.399739623]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 0 ... 1 1 1]\n",
      "inverse mask [0 0 1 ... 0 0 0]\n",
      "masked point level mse: [1.07985187 1.27753019 0 ... 1.70080888 0.691841543 0.399739623]\n",
      "inverse masked point level mse: [0 0 1.45847642 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.42332935\n",
      "Standard batch-level MSE using reduce_sum: 1.42332935\n",
      "Mask length: 20\n",
      "Inverse mask length: 12\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 22.3750896\n",
      "inverse masked sum: 23.1714516\n",
      " \n",
      "masked numerator: 22.3750896\n",
      "unmasked numerator: 23.1714516\n",
      "masked denominator: 20\n",
      "unmasked denominator: 12\n",
      "Masked batch level MSE: 1.11875451\n",
      "Inverse masked batch level MSE: 1.93095434\n",
      " \n",
      "point level mse: [4.67854834 2.72204423 0.501299798 ... 3.37346935 0.249445662 0.6334337]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 0 ... 0 1 1]\n",
      "inverse mask [1 1 1 ... 1 0 0]\n",
      "masked point level mse: [0 0 0 ... 0 0.249445662 0.6334337]\n",
      "inverse masked point level mse: [4.67854834 2.72204423 0.501299798 ... 3.37346935 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.10591722\n",
      "Standard batch-level MSE using reduce_sum: 1.10591722\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 16.639822\n",
      "inverse masked sum: 18.749527\n",
      " \n",
      "masked numerator: 16.639822\n",
      "unmasked numerator: 18.749527\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 0.756355524\n",
      "Inverse masked batch level MSE: 1.87495267\n",
      " \n",
      "point level mse: [0.210738257 1.56442678 0.627438664 ... 0.332500666 5.86251259 0.319996357]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 1 0 1]\n",
      "inverse mask [0 1 0 ... 0 1 0]\n",
      "masked point level mse: [0.210738257 0 0.627438664 ... 0.332500666 0 0.319996357]\n",
      "inverse masked point level mse: [0 1.56442678 0 ... 0 5.86251259 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.984017193\n",
      "Standard batch-level MSE using reduce_sum: 0.984017193\n",
      "Mask length: 19\n",
      "Inverse mask length: 13\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 11.9312515\n",
      "inverse masked sum: 19.5572987\n",
      " \n",
      "masked numerator: 11.9312515\n",
      "unmasked numerator: 19.5572987\n",
      "masked denominator: 19\n",
      "unmasked denominator: 13\n",
      "Masked batch level MSE: 0.627960622\n",
      "Inverse masked batch level MSE: 1.50440764\n",
      " \n",
      "point level mse: [0.271825463 0.526920378 0.425596386 ... 1.13604951 0.573447347 0.393568605]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 0 1 0]\n",
      "inverse mask [0 0 0 ... 1 0 1]\n",
      "masked point level mse: [0.271825463 0.526920378 0.425596386 ... 0 0.573447347 0]\n",
      "inverse masked point level mse: [0 0 0 ... 1.13604951 0 0.393568605]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.15542865\n",
      "Standard batch-level MSE using reduce_sum: 1.15542865\n",
      "Mask length: 15\n",
      "Inverse mask length: 17\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 8.77166939\n",
      "inverse masked sum: 28.2020493\n",
      " \n",
      "masked numerator: 8.77166939\n",
      "unmasked numerator: 28.2020493\n",
      "masked denominator: 15\n",
      "unmasked denominator: 17\n",
      "Masked batch level MSE: 0.584777951\n",
      "Inverse masked batch level MSE: 1.65894413\n",
      " \n",
      "point level mse: [0.335045129 0.285772026 0.849850893 ... 0.141117066 1.79766917 0.749254]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 1 0 1]\n",
      "inverse mask [1 0 0 ... 0 1 0]\n",
      "masked point level mse: [0 0.285772026 0.849850893 ... 0.141117066 0 0.749254]\n",
      "inverse masked point level mse: [0.335045129 0 0 ... 0 1.79766917 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.863020182\n",
      "Standard batch-level MSE using reduce_sum: 0.863020182\n",
      "Mask length: 23\n",
      "Inverse mask length: 9\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 11.5331936\n",
      "inverse masked sum: 16.0834541\n",
      " \n",
      "masked numerator: 11.5331936\n",
      "unmasked numerator: 16.0834541\n",
      "masked denominator: 23\n",
      "unmasked denominator: 9\n",
      "Masked batch level MSE: 0.501443207\n",
      "Inverse masked batch level MSE: 1.78705049\n",
      " \n",
      "point level mse: [2.2222321 0.323365688 0.405015439 ... 1.8578738 0.243313923 0.28031826]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 1 1 1]\n",
      "inverse mask [1 0 0 ... 0 0 0]\n",
      "masked point level mse: [0 0.323365688 0.405015439 ... 1.8578738 0.243313923 0.28031826]\n",
      "inverse masked point level mse: [2.2222321 0 0 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.11797428\n",
      "Standard batch-level MSE using reduce_sum: 1.11797428\n",
      "Mask length: 4\n",
      "Inverse mask length: 3\n",
      "n lables in batch: 7\n",
      " \n",
      "masked sum: 5.74483156\n",
      "inverse masked sum: 2.08098888\n",
      " \n",
      "masked numerator: 5.74483156\n",
      "unmasked numerator: 2.08098888\n",
      "masked denominator: 4\n",
      "unmasked denominator: 3\n",
      "Masked batch level MSE: 1.43620789\n",
      "Inverse masked batch level MSE: 0.693662941\n",
      " \n",
      "point level mse: [0.581541419 1.62076676 0.522374511 ... 0.867144525 3.22674489 0.374945313]\n",
      " \n",
      "mse shape: TensorShape([7])\n",
      "mask shape: TensorShape([7])\n",
      "inverse mask shape: TensorShape([7])\n",
      "mask [0 1 1 ... 0 1 1]\n",
      "inverse mask [1 0 0 ... 1 0 0]\n",
      "masked point level mse: [0 1.62076676 0.522374511 ... 0 3.22674489 0.374945313]\n",
      "inverse masked point level mse: [0.581541419 0 0 ... 0.867144525 0 0]\n",
      "Epoch 4 completed with final batch loss: [0.        1.6207668 0.5223745 0.        0.        3.226745  0.3749453]\n",
      "Epoch 5/10\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.11149025\n",
      "Standard batch-level MSE using reduce_sum: 1.11149025\n",
      "Mask length: 19\n",
      "Inverse mask length: 13\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 9.93817329\n",
      "inverse masked sum: 25.6295147\n",
      " \n",
      "masked numerator: 9.93817329\n",
      "unmasked numerator: 25.6295147\n",
      "masked denominator: 19\n",
      "unmasked denominator: 13\n",
      "Masked batch level MSE: 0.523061752\n",
      "Inverse masked batch level MSE: 1.97150111\n",
      " \n",
      "point level mse: [0.499020725 0.320490897 1.50562584 ... 1.55528128 2.48284364 0.355371773]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 0 ... 0 0 1]\n",
      "inverse mask [0 0 1 ... 1 1 0]\n",
      "masked point level mse: [0.499020725 0.320490897 0 ... 0 0 0.355371773]\n",
      "inverse masked point level mse: [0 0 1.50562584 ... 1.55528128 2.48284364 0]\n",
      "Step 0: Loss = [0.49902073 0.3204909  0.         0.3446123  1.0966145  0.6439364\n",
      " 0.26715162 0.         0.         0.52790475 0.44161    0.\n",
      " 0.         0.32763878 0.75132626 0.         0.5202199  0.21948686\n",
      " 0.31904575 1.003683   0.13866733 0.5489529  0.         0.\n",
      " 0.         1.1433324  0.46910718 0.         0.         0.\n",
      " 0.         0.35537177]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.01481688\n",
      "Standard batch-level MSE using reduce_sum: 1.01481688\n",
      "Mask length: 17\n",
      "Inverse mask length: 15\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 7.4330759\n",
      "inverse masked sum: 25.0410671\n",
      " \n",
      "masked numerator: 7.4330759\n",
      "unmasked numerator: 25.0410671\n",
      "masked denominator: 17\n",
      "unmasked denominator: 15\n",
      "Masked batch level MSE: 0.437239766\n",
      "Inverse masked batch level MSE: 1.66940451\n",
      " \n",
      "point level mse: [4.59543657 1.18060756 0.24946776 ... 0.406014383 1.38483691 0.695873678]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 1 0 1]\n",
      "inverse mask [1 0 0 ... 0 1 0]\n",
      "masked point level mse: [0 1.18060756 0.24946776 ... 0.406014383 0 0.695873678]\n",
      "inverse masked point level mse: [4.59543657 0 0 ... 0 1.38483691 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.10808921\n",
      "Standard batch-level MSE using reduce_sum: 1.10808921\n",
      "Mask length: 20\n",
      "Inverse mask length: 12\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 11.0299683\n",
      "inverse masked sum: 24.4288864\n",
      " \n",
      "masked numerator: 11.0299683\n",
      "unmasked numerator: 24.4288864\n",
      "masked denominator: 20\n",
      "unmasked denominator: 12\n",
      "Masked batch level MSE: 0.551498413\n",
      "Inverse masked batch level MSE: 2.03574061\n",
      " \n",
      "point level mse: [0.694288969 0.899555922 0.65525 ... 0.289991021 0.453309476 0.376299411]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 0 ... 1 1 1]\n",
      "inverse mask [0 0 1 ... 0 0 0]\n",
      "masked point level mse: [0.694288969 0.899555922 0 ... 0.289991021 0.453309476 0.376299411]\n",
      "inverse masked point level mse: [0 0 0.65525 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.22743857\n",
      "Standard batch-level MSE using reduce_sum: 1.22743857\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 17.8812218\n",
      "inverse masked sum: 21.3968124\n",
      " \n",
      "masked numerator: 17.8812218\n",
      "unmasked numerator: 21.3968124\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 0.812782824\n",
      "Inverse masked batch level MSE: 2.13968134\n",
      " \n",
      "point level mse: [1.42056477 0.618953705 0.858106 ... 0.682241 0.314428777 1.65245986]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 0 1 1]\n",
      "inverse mask [0 0 0 ... 1 0 0]\n",
      "masked point level mse: [1.42056477 0.618953705 0.858106 ... 0 0.314428777 1.65245986]\n",
      "inverse masked point level mse: [0 0 0 ... 0.682241 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 2.02881\n",
      "Standard batch-level MSE using reduce_sum: 2.02881\n",
      "Mask length: 23\n",
      "Inverse mask length: 9\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 21.2621346\n",
      "inverse masked sum: 43.6597862\n",
      " \n",
      "masked numerator: 21.2621346\n",
      "unmasked numerator: 43.6597862\n",
      "masked denominator: 23\n",
      "unmasked denominator: 9\n",
      "Masked batch level MSE: 0.924440622\n",
      "Inverse masked batch level MSE: 4.85108757\n",
      " \n",
      "point level mse: [0.82815814 0.390622318 0.819716096 ... 0.691500366 0.510271251 1.13556886]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 1 1 0]\n",
      "inverse mask [0 0 0 ... 0 0 1]\n",
      "masked point level mse: [0.82815814 0.390622318 0.819716096 ... 0.691500366 0.510271251 0]\n",
      "inverse masked point level mse: [0 0 0 ... 0 0 1.13556886]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.13768125\n",
      "Standard batch-level MSE using reduce_sum: 1.13768125\n",
      "Mask length: 23\n",
      "Inverse mask length: 9\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 16.7811317\n",
      "inverse masked sum: 19.6246719\n",
      " \n",
      "masked numerator: 16.7811317\n",
      "unmasked numerator: 19.6246719\n",
      "masked denominator: 23\n",
      "unmasked denominator: 9\n",
      "Masked batch level MSE: 0.729614437\n",
      "Inverse masked batch level MSE: 2.1805191\n",
      " \n",
      "point level mse: [2.23163223 1.0924499 2.20747256 ... 0.539331 0.3515369 5.86913204]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 0 ... 1 1 0]\n",
      "inverse mask [1 1 1 ... 0 0 1]\n",
      "masked point level mse: [0 0 0 ... 0.539331 0.3515369 0]\n",
      "inverse masked point level mse: [2.23163223 1.0924499 2.20747256 ... 0 0 5.86913204]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.30034947\n",
      "Standard batch-level MSE using reduce_sum: 1.30034947\n",
      "Mask length: 19\n",
      "Inverse mask length: 13\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 12.9652119\n",
      "inverse masked sum: 28.6459713\n",
      " \n",
      "masked numerator: 12.9652119\n",
      "unmasked numerator: 28.6459713\n",
      "masked denominator: 19\n",
      "unmasked denominator: 13\n",
      "Masked batch level MSE: 0.682379544\n",
      "Inverse masked batch level MSE: 2.20353627\n",
      " \n",
      "point level mse: [0.41749981 0.950032234 0.613977849 ... 0.858897924 0.471319079 0.88407743]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 1 1 1]\n",
      "inverse mask [0 0 0 ... 0 0 0]\n",
      "masked point level mse: [0.41749981 0.950032234 0.613977849 ... 0.858897924 0.471319079 0.88407743]\n",
      "inverse masked point level mse: [0 0 0 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.82034028\n",
      "Standard batch-level MSE using reduce_sum: 1.82034028\n",
      "Mask length: 17\n",
      "Inverse mask length: 15\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 24.9256229\n",
      "inverse masked sum: 33.325264\n",
      " \n",
      "masked numerator: 24.9256229\n",
      "unmasked numerator: 33.325264\n",
      "masked denominator: 17\n",
      "unmasked denominator: 15\n",
      "Masked batch level MSE: 1.46621311\n",
      "Inverse masked batch level MSE: 2.22168422\n",
      " \n",
      "point level mse: [3.65971971 0.245980263 2.85213375 ... 13.3057289 0.334088713 1.65858269]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 1 1 0]\n",
      "inverse mask [0 0 0 ... 0 0 1]\n",
      "masked point level mse: [3.65971971 0.245980263 2.85213375 ... 13.3057289 0.334088713 0]\n",
      "inverse masked point level mse: [0 0 0 ... 0 0 1.65858269]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.03929448\n",
      "Standard batch-level MSE using reduce_sum: 1.03929448\n",
      "Mask length: 17\n",
      "Inverse mask length: 15\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 8.58895874\n",
      "inverse masked sum: 24.6684647\n",
      " \n",
      "masked numerator: 8.58895874\n",
      "unmasked numerator: 24.6684647\n",
      "masked denominator: 17\n",
      "unmasked denominator: 15\n",
      "Masked batch level MSE: 0.505232871\n",
      "Inverse masked batch level MSE: 1.64456427\n",
      " \n",
      "point level mse: [0.389021218 0.906251907 1.92858875 ... 1.10024953 0.469681293 1.11577117]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 0 ... 0 1 1]\n",
      "inverse mask [1 1 1 ... 1 0 0]\n",
      "masked point level mse: [0 0 0 ... 0 0.469681293 1.11577117]\n",
      "inverse masked point level mse: [0.389021218 0.906251907 1.92858875 ... 1.10024953 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.800695658\n",
      "Standard batch-level MSE using reduce_sum: 0.800695658\n",
      "Mask length: 24\n",
      "Inverse mask length: 8\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 14.4211464\n",
      "inverse masked sum: 11.2011147\n",
      " \n",
      "masked numerator: 14.4211464\n",
      "unmasked numerator: 11.2011147\n",
      "masked denominator: 24\n",
      "unmasked denominator: 8\n",
      "Masked batch level MSE: 0.6008811\n",
      "Inverse masked batch level MSE: 1.40013933\n",
      " \n",
      "point level mse: [0.405529141 0.337792 1.28610992 ... 0.36254397 0.267575949 0.332213193]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 1 1 1]\n",
      "inverse mask [0 0 0 ... 0 0 0]\n",
      "masked point level mse: [0.405529141 0.337792 1.28610992 ... 0.36254397 0.267575949 0.332213193]\n",
      "inverse masked point level mse: [0 0 0 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 2.58951139\n",
      "Standard batch-level MSE using reduce_sum: 2.58951139\n",
      "Mask length: 20\n",
      "Inverse mask length: 12\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 12.0137081\n",
      "inverse masked sum: 70.8506622\n",
      " \n",
      "masked numerator: 12.0137081\n",
      "unmasked numerator: 70.8506622\n",
      "masked denominator: 20\n",
      "unmasked denominator: 12\n",
      "Masked batch level MSE: 0.600685418\n",
      "Inverse masked batch level MSE: 5.90422201\n",
      " \n",
      "point level mse: [0.249646023 0.539769113 3.75606394 ... 0.284705937 2.60145283 8.48325157]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 0 ... 1 1 0]\n",
      "inverse mask [0 0 1 ... 0 0 1]\n",
      "masked point level mse: [0.249646023 0.539769113 0 ... 0.284705937 2.60145283 0]\n",
      "inverse masked point level mse: [0 0 3.75606394 ... 0 0 8.48325157]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.94999826\n",
      "Standard batch-level MSE using reduce_sum: 0.94999826\n",
      "Mask length: 15\n",
      "Inverse mask length: 17\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 7.65998602\n",
      "inverse masked sum: 22.7399559\n",
      " \n",
      "masked numerator: 7.65998602\n",
      "unmasked numerator: 22.7399559\n",
      "masked denominator: 15\n",
      "unmasked denominator: 17\n",
      "Masked batch level MSE: 0.510665715\n",
      "Inverse masked batch level MSE: 1.33764446\n",
      " \n",
      "point level mse: [0.321865857 0.181849465 1.59022689 ... 0.276364714 0.300348073 0.29354465]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 0 ... 1 1 0]\n",
      "inverse mask [0 0 1 ... 0 0 1]\n",
      "masked point level mse: [0.321865857 0.181849465 0 ... 0.276364714 0.300348073 0]\n",
      "inverse masked point level mse: [0 0 1.59022689 ... 0 0 0.29354465]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.920921147\n",
      "Standard batch-level MSE using reduce_sum: 0.920921147\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 13.1773272\n",
      "inverse masked sum: 16.2921486\n",
      " \n",
      "masked numerator: 13.1773272\n",
      "unmasked numerator: 16.2921486\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 0.5989694\n",
      "Inverse masked batch level MSE: 1.62921488\n",
      " \n",
      "point level mse: [0.84989965 0.31407091 0.154703915 ... 0.520339668 3.09300971 0.719626427]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 1 0 1]\n",
      "inverse mask [0 0 0 ... 0 1 0]\n",
      "masked point level mse: [0.84989965 0.31407091 0.154703915 ... 0.520339668 0 0.719626427]\n",
      "inverse masked point level mse: [0 0 0 ... 0 3.09300971 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.959224\n",
      "Standard batch-level MSE using reduce_sum: 0.959224\n",
      "Mask length: 23\n",
      "Inverse mask length: 9\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 15.077076\n",
      "inverse masked sum: 15.6180906\n",
      " \n",
      "masked numerator: 15.077076\n",
      "unmasked numerator: 15.6180906\n",
      "masked denominator: 23\n",
      "unmasked denominator: 9\n",
      "Masked batch level MSE: 0.655525\n",
      "Inverse masked batch level MSE: 1.73534346\n",
      " \n",
      "point level mse: [0.508539796 3.46367192 0.144491643 ... 0.645923376 0.243087828 0.473574519]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 1 0 1]\n",
      "inverse mask [0 1 0 ... 0 1 0]\n",
      "masked point level mse: [0.508539796 0 0.144491643 ... 0.645923376 0 0.473574519]\n",
      "inverse masked point level mse: [0 3.46367192 0 ... 0 0.243087828 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.505166054\n",
      "Standard batch-level MSE using reduce_sum: 0.505166054\n",
      "Mask length: 5\n",
      "Inverse mask length: 2\n",
      "n lables in batch: 7\n",
      " \n",
      "masked sum: 1.64970911\n",
      "inverse masked sum: 1.88645315\n",
      " \n",
      "masked numerator: 1.64970911\n",
      "unmasked numerator: 1.88645315\n",
      "masked denominator: 5\n",
      "unmasked denominator: 2\n",
      "Masked batch level MSE: 0.329941809\n",
      "Inverse masked batch level MSE: 0.943226576\n",
      " \n",
      "point level mse: [0.300367743 0.355492204 0.332597136 ... 0.250963926 0.410288066 0.67537111]\n",
      " \n",
      "mse shape: TensorShape([7])\n",
      "mask shape: TensorShape([7])\n",
      "inverse mask shape: TensorShape([7])\n",
      "mask [1 1 1 ... 1 1 0]\n",
      "inverse mask [0 0 0 ... 0 0 1]\n",
      "masked point level mse: [0.300367743 0.355492204 0.332597136 ... 0.250963926 0.410288066 0]\n",
      "inverse masked point level mse: [0 0 0 ... 0 0 0.67537111]\n",
      "Epoch 5 completed with final batch loss: [0.30036774 0.3554922  0.33259714 0.         0.25096393 0.41028807\n",
      " 0.        ]\n",
      "Epoch 6/10\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.795835912\n",
      "Standard batch-level MSE using reduce_sum: 0.795835912\n",
      "Mask length: 21\n",
      "Inverse mask length: 11\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 8.38410854\n",
      "inverse masked sum: 17.0826416\n",
      " \n",
      "masked numerator: 8.38410854\n",
      "unmasked numerator: 17.0826416\n",
      "masked denominator: 21\n",
      "unmasked denominator: 11\n",
      "Masked batch level MSE: 0.399243265\n",
      "Inverse masked batch level MSE: 1.55296743\n",
      " \n",
      "point level mse: [0.371054947 1.20426762 0.873366773 ... 0.497903913 0.547200739 3.07979345]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 0 ... 0 1 0]\n",
      "inverse mask [0 1 1 ... 1 0 1]\n",
      "masked point level mse: [0.371054947 0 0 ... 0 0.547200739 0]\n",
      "inverse masked point level mse: [0 1.20426762 0.873366773 ... 0.497903913 0 3.07979345]\n",
      "Step 0: Loss = [0.37105495 0.         0.         0.13616662 0.         0.19814482\n",
      " 0.31425953 0.5622763  0.         0.2666006  0.28138912 0.30372858\n",
      " 0.         0.4766521  0.36968285 0.5862074  0.3051938  0.2613928\n",
      " 0.26846492 0.24652822 0.47823256 0.         0.28414103 0.9876858\n",
      " 0.5281361  0.         0.6109701  0.         0.         0.\n",
      " 0.54720074 0.        ]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.62271452\n",
      "Standard batch-level MSE using reduce_sum: 1.62271452\n",
      "Mask length: 18\n",
      "Inverse mask length: 14\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 17.2712421\n",
      "inverse masked sum: 34.6556244\n",
      " \n",
      "masked numerator: 17.2712421\n",
      "unmasked numerator: 34.6556244\n",
      "masked denominator: 18\n",
      "unmasked denominator: 14\n",
      "Masked batch level MSE: 0.959513426\n",
      "Inverse masked batch level MSE: 2.47540164\n",
      " \n",
      "point level mse: [0.201866597 3.60513377 1.00066185 ... 0.485448062 0.266993731 0.381370842]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 0 1 1]\n",
      "inverse mask [0 1 0 ... 1 0 0]\n",
      "masked point level mse: [0.201866597 0 1.00066185 ... 0 0.266993731 0.381370842]\n",
      "inverse masked point level mse: [0 3.60513377 0 ... 0.485448062 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.8930493\n",
      "Standard batch-level MSE using reduce_sum: 0.8930493\n",
      "Mask length: 23\n",
      "Inverse mask length: 9\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 13.3785734\n",
      "inverse masked sum: 15.1990051\n",
      " \n",
      "masked numerator: 13.3785734\n",
      "unmasked numerator: 15.1990051\n",
      "masked denominator: 23\n",
      "unmasked denominator: 9\n",
      "Masked batch level MSE: 0.581677079\n",
      "Inverse masked batch level MSE: 1.6887784\n",
      " \n",
      "point level mse: [0.69944644 0.498534858 1.17764449 ... 0.330742866 0.426527172 0.597662]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 0 ... 1 1 0]\n",
      "inverse mask [1 0 1 ... 0 0 1]\n",
      "masked point level mse: [0 0.498534858 0 ... 0.330742866 0.426527172 0]\n",
      "inverse masked point level mse: [0.69944644 0 1.17764449 ... 0 0 0.597662]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.86144769\n",
      "Standard batch-level MSE using reduce_sum: 1.86144769\n",
      "Mask length: 20\n",
      "Inverse mask length: 12\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 16.1802502\n",
      "inverse masked sum: 43.3860741\n",
      " \n",
      "masked numerator: 16.1802502\n",
      "unmasked numerator: 43.3860741\n",
      "masked denominator: 20\n",
      "unmasked denominator: 12\n",
      "Masked batch level MSE: 0.809012532\n",
      "Inverse masked batch level MSE: 3.61550617\n",
      " \n",
      "point level mse: [0.762891889 0.214847371 1.54997838 ... 0.32191509 25.4796 0.514051735]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 0 ... 1 0 1]\n",
      "inverse mask [1 0 1 ... 0 1 0]\n",
      "masked point level mse: [0 0.214847371 0 ... 0.32191509 0 0.514051735]\n",
      "inverse masked point level mse: [0.762891889 0 1.54997838 ... 0 25.4796 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.2095747\n",
      "Standard batch-level MSE using reduce_sum: 1.2095747\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 13.0592813\n",
      "inverse masked sum: 25.64711\n",
      " \n",
      "masked numerator: 13.0592813\n",
      "unmasked numerator: 25.64711\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 0.593603671\n",
      "Inverse masked batch level MSE: 2.56471109\n",
      " \n",
      "point level mse: [0.354234397 0.4741 1.1480664 ... 2.62540889 2.7452004 0.502599657]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 0 ... 0 1 1]\n",
      "inverse mask [1 1 1 ... 1 0 0]\n",
      "masked point level mse: [0 0 0 ... 0 2.7452004 0.502599657]\n",
      "inverse masked point level mse: [0.354234397 0.4741 1.1480664 ... 2.62540889 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.908772349\n",
      "Standard batch-level MSE using reduce_sum: 0.908772349\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 10.918354\n",
      "inverse masked sum: 18.1623611\n",
      " \n",
      "masked numerator: 10.918354\n",
      "unmasked numerator: 18.1623611\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 0.496288806\n",
      "Inverse masked batch level MSE: 1.81623614\n",
      " \n",
      "point level mse: [2.03831339 0.335900187 0.476876169 ... 0.256567806 0.499237508 0.438657314]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 1 1 1]\n",
      "inverse mask [1 0 0 ... 0 0 0]\n",
      "masked point level mse: [0 0.335900187 0.476876169 ... 0.256567806 0.499237508 0.438657314]\n",
      "inverse masked point level mse: [2.03831339 0 0 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.20063818\n",
      "Standard batch-level MSE using reduce_sum: 1.20063818\n",
      "Mask length: 23\n",
      "Inverse mask length: 9\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 16.1563931\n",
      "inverse masked sum: 22.2640305\n",
      " \n",
      "masked numerator: 16.1563931\n",
      "unmasked numerator: 22.2640305\n",
      "masked denominator: 23\n",
      "unmasked denominator: 9\n",
      "Masked batch level MSE: 0.702451885\n",
      "Inverse masked batch level MSE: 2.47378111\n",
      " \n",
      "point level mse: [0.503233254 3.11388278 2.59815812 ... 1.59145963 4.89155293 0.404595703]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 0 0 1]\n",
      "inverse mask [0 0 0 ... 1 1 0]\n",
      "masked point level mse: [0.503233254 3.11388278 2.59815812 ... 0 0 0.404595703]\n",
      "inverse masked point level mse: [0 0 0 ... 1.59145963 4.89155293 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.82075429\n",
      "Standard batch-level MSE using reduce_sum: 0.82075429\n",
      "Mask length: 21\n",
      "Inverse mask length: 11\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 11.3264914\n",
      "inverse masked sum: 14.9376478\n",
      " \n",
      "masked numerator: 11.3264914\n",
      "unmasked numerator: 14.9376478\n",
      "masked denominator: 21\n",
      "unmasked denominator: 11\n",
      "Masked batch level MSE: 0.539356709\n",
      "Inverse masked batch level MSE: 1.35796797\n",
      " \n",
      "point level mse: [0.386398256 0.370021522 1.07991564 ... 0.246154502 1.1356529 0.583354414]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 0 0 1]\n",
      "inverse mask [1 0 0 ... 1 1 0]\n",
      "masked point level mse: [0 0.370021522 1.07991564 ... 0 0 0.583354414]\n",
      "inverse masked point level mse: [0.386398256 0 0 ... 0.246154502 1.1356529 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.2331419\n",
      "Standard batch-level MSE using reduce_sum: 1.2331419\n",
      "Mask length: 19\n",
      "Inverse mask length: 13\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 7.64964199\n",
      "inverse masked sum: 31.8109016\n",
      " \n",
      "masked numerator: 7.64964199\n",
      "unmasked numerator: 31.8109016\n",
      "masked denominator: 19\n",
      "unmasked denominator: 13\n",
      "Masked batch level MSE: 0.402612746\n",
      "Inverse masked batch level MSE: 2.4469924\n",
      " \n",
      "point level mse: [1.00041842 0.42964533 2.75882959 ... 1.84548962 0.301643252 0.315932781]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 0 ... 0 1 1]\n",
      "inverse mask [1 0 1 ... 1 0 0]\n",
      "masked point level mse: [0 0.42964533 0 ... 0 0.301643252 0.315932781]\n",
      "inverse masked point level mse: [1.00041842 0 2.75882959 ... 1.84548962 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.95351923\n",
      "Standard batch-level MSE using reduce_sum: 1.95351923\n",
      "Mask length: 17\n",
      "Inverse mask length: 15\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 12.0358295\n",
      "inverse masked sum: 50.4767838\n",
      " \n",
      "masked numerator: 12.0358295\n",
      "unmasked numerator: 50.4767838\n",
      "masked denominator: 17\n",
      "unmasked denominator: 15\n",
      "Masked batch level MSE: 0.70799\n",
      "Inverse masked batch level MSE: 3.36511898\n",
      " \n",
      "point level mse: [2.21675539 0.697706878 0.538560927 ... 1.66076612 0.314043075 0.479008287]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 1 ... 0 1 1]\n",
      "inverse mask [1 1 0 ... 1 0 0]\n",
      "masked point level mse: [0 0 0.538560927 ... 0 0.314043075 0.479008287]\n",
      "inverse masked point level mse: [2.21675539 0.697706878 0 ... 1.66076612 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.980765343\n",
      "Standard batch-level MSE using reduce_sum: 0.980765343\n",
      "Mask length: 17\n",
      "Inverse mask length: 15\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 11.1944408\n",
      "inverse masked sum: 20.190052\n",
      " \n",
      "masked numerator: 11.1944408\n",
      "unmasked numerator: 20.190052\n",
      "masked denominator: 17\n",
      "unmasked denominator: 15\n",
      "Masked batch level MSE: 0.658496499\n",
      "Inverse masked batch level MSE: 1.34600341\n",
      " \n",
      "point level mse: [0.519621909 1.43137884 0.722792387 ... 1.11469054 0.517609 0.612346292]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 1 ... 0 0 0]\n",
      "inverse mask [1 1 0 ... 1 1 1]\n",
      "masked point level mse: [0 0 0.722792387 ... 0 0 0]\n",
      "inverse masked point level mse: [0.519621909 1.43137884 0 ... 1.11469054 0.517609 0.612346292]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.54120016\n",
      "Standard batch-level MSE using reduce_sum: 1.54120016\n",
      "Mask length: 18\n",
      "Inverse mask length: 14\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 10.6906223\n",
      "inverse masked sum: 38.6277847\n",
      " \n",
      "masked numerator: 10.6906223\n",
      "unmasked numerator: 38.6277847\n",
      "masked denominator: 18\n",
      "unmasked denominator: 14\n",
      "Masked batch level MSE: 0.59392345\n",
      "Inverse masked batch level MSE: 2.75912738\n",
      " \n",
      "point level mse: [0.136812404 0.370051414 1.04577935 ... 1.8648355 0.747619629 1.18824017]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 0 ... 0 1 0]\n",
      "inverse mask [0 0 1 ... 1 0 1]\n",
      "masked point level mse: [0.136812404 0.370051414 0 ... 0 0.747619629 0]\n",
      "inverse masked point level mse: [0 0 1.04577935 ... 1.8648355 0 1.18824017]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.56847823\n",
      "Standard batch-level MSE using reduce_sum: 1.56847823\n",
      "Mask length: 20\n",
      "Inverse mask length: 12\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 27.1097813\n",
      "inverse masked sum: 23.081522\n",
      " \n",
      "masked numerator: 27.1097813\n",
      "unmasked numerator: 23.081522\n",
      "masked denominator: 20\n",
      "unmasked denominator: 12\n",
      "Masked batch level MSE: 1.35548902\n",
      "Inverse masked batch level MSE: 1.92346013\n",
      " \n",
      "point level mse: [0.357700616 3.13824344 0.619022608 ... 0.601035118 0.748361647 1.27042198]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 1 1 1]\n",
      "inverse mask [0 1 0 ... 0 0 0]\n",
      "masked point level mse: [0.357700616 0 0.619022608 ... 0.601035118 0.748361647 1.27042198]\n",
      "inverse masked point level mse: [0 3.13824344 0 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.04190159\n",
      "Standard batch-level MSE using reduce_sum: 1.04190159\n",
      "Mask length: 20\n",
      "Inverse mask length: 12\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 8.04749584\n",
      "inverse masked sum: 25.293354\n",
      " \n",
      "masked numerator: 8.04749584\n",
      "unmasked numerator: 25.293354\n",
      "masked denominator: 20\n",
      "unmasked denominator: 12\n",
      "Masked batch level MSE: 0.402374804\n",
      "Inverse masked batch level MSE: 2.1077795\n",
      " \n",
      "point level mse: [1.13170302 0.569458246 0.377807736 ... 2.54986262 1.47916794 1.08753788]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 0 0 0]\n",
      "inverse mask [1 0 0 ... 1 1 1]\n",
      "masked point level mse: [0 0.569458246 0.377807736 ... 0 0 0]\n",
      "inverse masked point level mse: [1.13170302 0 0 ... 2.54986262 1.47916794 1.08753788]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.69891739\n",
      "Standard batch-level MSE using reduce_sum: 1.69891739\n",
      "Mask length: 5\n",
      "Inverse mask length: 2\n",
      "n lables in batch: 7\n",
      " \n",
      "masked sum: 2.70891809\n",
      "inverse masked sum: 9.18350506\n",
      " \n",
      "masked numerator: 2.70891809\n",
      "unmasked numerator: 9.18350506\n",
      "masked denominator: 5\n",
      "unmasked denominator: 2\n",
      "Masked batch level MSE: 0.541783631\n",
      "Inverse masked batch level MSE: 4.59175253\n",
      " \n",
      "point level mse: [0.577417195 1.01062119 0.448991179 ... 0.205259055 0.682661712 0.466629148]\n",
      " \n",
      "mse shape: TensorShape([7])\n",
      "mask shape: TensorShape([7])\n",
      "inverse mask shape: TensorShape([7])\n",
      "mask [1 1 1 ... 1 0 1]\n",
      "inverse mask [0 0 0 ... 0 1 0]\n",
      "masked point level mse: [0.577417195 1.01062119 0.448991179 ... 0.205259055 0 0.466629148]\n",
      "inverse masked point level mse: [0 0 0 ... 0 0.682661712 0]\n",
      "Epoch 6 completed with final batch loss: [0.5774172  1.0106212  0.44899118 0.         0.20525905 0.\n",
      " 0.46662915]\n",
      "Epoch 7/10\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.71671236\n",
      "Standard batch-level MSE using reduce_sum: 1.71671236\n",
      "Mask length: 18\n",
      "Inverse mask length: 14\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 10.7494183\n",
      "inverse masked sum: 44.185379\n",
      " \n",
      "masked numerator: 10.7494183\n",
      "unmasked numerator: 44.185379\n",
      "masked denominator: 18\n",
      "unmasked denominator: 14\n",
      "Masked batch level MSE: 0.597189903\n",
      "Inverse masked batch level MSE: 3.1560986\n",
      " \n",
      "point level mse: [1.95153916 1.39509821 0.58439815 ... 1.2169416 3.24931979 4.86003876]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 0 0 0]\n",
      "inverse mask [1 0 0 ... 1 1 1]\n",
      "masked point level mse: [0 1.39509821 0.58439815 ... 0 0 0]\n",
      "inverse masked point level mse: [1.95153916 0 0 ... 1.2169416 3.24931979 4.86003876]\n",
      "Step 0: Loss = [0.         1.3950982  0.58439815 0.3081872  0.15511431 0.\n",
      " 0.24971047 0.60953003 0.39053425 0.         0.         2.5816033\n",
      " 0.14052778 0.         0.3925243  0.         0.         0.29029748\n",
      " 0.         0.2833814  0.25773987 0.26814458 0.         1.2639468\n",
      " 0.43507704 0.         0.45923272 0.68437016 0.         0.\n",
      " 0.         0.        ]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.13091969\n",
      "Standard batch-level MSE using reduce_sum: 1.13091969\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 22.4497929\n",
      "inverse masked sum: 13.7396383\n",
      " \n",
      "masked numerator: 22.4497929\n",
      "unmasked numerator: 13.7396383\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 1.02044511\n",
      "Inverse masked batch level MSE: 1.37396383\n",
      " \n",
      "point level mse: [0.273615152 0.463381 0.311396807 ... 1.81409502 0.287926108 1.70926583]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 0 1 0]\n",
      "inverse mask [0 0 0 ... 1 0 1]\n",
      "masked point level mse: [0.273615152 0.463381 0.311396807 ... 0 0.287926108 0]\n",
      "inverse masked point level mse: [0 0 0 ... 1.81409502 0 1.70926583]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.24116647\n",
      "Standard batch-level MSE using reduce_sum: 1.24116647\n",
      "Mask length: 19\n",
      "Inverse mask length: 13\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 11.9365845\n",
      "inverse masked sum: 27.7807426\n",
      " \n",
      "masked numerator: 11.9365845\n",
      "unmasked numerator: 27.7807426\n",
      "masked denominator: 19\n",
      "unmasked denominator: 13\n",
      "Masked batch level MSE: 0.628241301\n",
      "Inverse masked batch level MSE: 2.1369803\n",
      " \n",
      "point level mse: [0.129333064 0.379427373 1.17345846 ... 1.04117918 0.337118179 0.723687291]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 0 ... 1 1 0]\n",
      "inverse mask [0 0 1 ... 0 0 1]\n",
      "masked point level mse: [0.129333064 0.379427373 0 ... 1.04117918 0.337118179 0]\n",
      "inverse masked point level mse: [0 0 1.17345846 ... 0 0 0.723687291]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.14364576\n",
      "Standard batch-level MSE using reduce_sum: 1.14364576\n",
      "Mask length: 20\n",
      "Inverse mask length: 12\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 9.50805283\n",
      "inverse masked sum: 27.0886116\n",
      " \n",
      "masked numerator: 9.50805283\n",
      "unmasked numerator: 27.0886116\n",
      "masked denominator: 20\n",
      "unmasked denominator: 12\n",
      "Masked batch level MSE: 0.475402653\n",
      "Inverse masked batch level MSE: 2.2573843\n",
      " \n",
      "point level mse: [1.88667619 0.146524131 0.60702157 ... 0.227656201 0.292666942 1.64164317]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 1 1 0]\n",
      "inverse mask [1 0 0 ... 0 0 1]\n",
      "masked point level mse: [0 0.146524131 0.60702157 ... 0.227656201 0.292666942 0]\n",
      "inverse masked point level mse: [1.88667619 0 0 ... 0 0 1.64164317]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.23672628\n",
      "Standard batch-level MSE using reduce_sum: 1.23672628\n",
      "Mask length: 17\n",
      "Inverse mask length: 15\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 7.79180431\n",
      "inverse masked sum: 31.7834396\n",
      " \n",
      "masked numerator: 7.79180431\n",
      "unmasked numerator: 31.7834396\n",
      "masked denominator: 17\n",
      "unmasked denominator: 15\n",
      "Masked batch level MSE: 0.45834142\n",
      "Inverse masked batch level MSE: 2.11889601\n",
      " \n",
      "point level mse: [3.32956362 0.564175427 0.611381888 ... 4.33797598 0.809232712 1.94437921]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 0 ... 0 1 0]\n",
      "inverse mask [1 0 1 ... 1 0 1]\n",
      "masked point level mse: [0 0.564175427 0 ... 0 0.809232712 0]\n",
      "inverse masked point level mse: [3.32956362 0 0.611381888 ... 4.33797598 0 1.94437921]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.1294868\n",
      "Standard batch-level MSE using reduce_sum: 1.1294868\n",
      "Mask length: 19\n",
      "Inverse mask length: 13\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 8.95810223\n",
      "inverse masked sum: 27.1854744\n",
      " \n",
      "masked numerator: 8.95810223\n",
      "unmasked numerator: 27.1854744\n",
      "masked denominator: 19\n",
      "unmasked denominator: 13\n",
      "Masked batch level MSE: 0.471479058\n",
      "Inverse masked batch level MSE: 2.09119034\n",
      " \n",
      "point level mse: [0.451839238 3.63133073 0.674139142 ... 0.828420281 0.223371699 1.25209856]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 1 0 0]\n",
      "inverse mask [0 1 0 ... 0 1 1]\n",
      "masked point level mse: [0.451839238 0 0.674139142 ... 0.828420281 0 0]\n",
      "inverse masked point level mse: [0 3.63133073 0 ... 0 0.223371699 1.25209856]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.81214142\n",
      "Standard batch-level MSE using reduce_sum: 1.81214142\n",
      "Mask length: 16\n",
      "Inverse mask length: 16\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 8.68053436\n",
      "inverse masked sum: 49.3079948\n",
      " \n",
      "masked numerator: 8.68053436\n",
      "unmasked numerator: 49.3079948\n",
      "masked denominator: 16\n",
      "unmasked denominator: 16\n",
      "Masked batch level MSE: 0.542533398\n",
      "Inverse masked batch level MSE: 3.08174968\n",
      " \n",
      "point level mse: [1.89369762 0.449936122 1.7677871 ... 0.263033539 1.17544794 1.49831307]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 0 ... 1 0 0]\n",
      "inverse mask [1 1 1 ... 0 1 1]\n",
      "masked point level mse: [0 0 0 ... 0.263033539 0 0]\n",
      "inverse masked point level mse: [1.89369762 0.449936122 1.7677871 ... 0 1.17544794 1.49831307]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.22563219\n",
      "Standard batch-level MSE using reduce_sum: 1.22563219\n",
      "Mask length: 21\n",
      "Inverse mask length: 11\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 11.4569712\n",
      "inverse masked sum: 27.7632561\n",
      " \n",
      "masked numerator: 11.4569712\n",
      "unmasked numerator: 27.7632561\n",
      "masked denominator: 21\n",
      "unmasked denominator: 11\n",
      "Masked batch level MSE: 0.545570076\n",
      "Inverse masked batch level MSE: 2.52393246\n",
      " \n",
      "point level mse: [0.428830326 5.2806592 0.589966178 ... 1.15578854 2.15786219 0.380118549]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 1 0 1]\n",
      "inverse mask [0 1 0 ... 0 1 0]\n",
      "masked point level mse: [0.428830326 0 0.589966178 ... 1.15578854 0 0.380118549]\n",
      "inverse masked point level mse: [0 5.2806592 0 ... 0 2.15786219 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.08615708\n",
      "Standard batch-level MSE using reduce_sum: 1.08615708\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 11.3078651\n",
      "inverse masked sum: 23.4491634\n",
      " \n",
      "masked numerator: 11.3078651\n",
      "unmasked numerator: 23.4491634\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 0.513993859\n",
      "Inverse masked batch level MSE: 2.34491634\n",
      " \n",
      "point level mse: [0.395640045 0.499699265 0.276671946 ... 0.630601108 0.170608789 0.33737433]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 1 1 1]\n",
      "inverse mask [0 0 0 ... 0 0 0]\n",
      "masked point level mse: [0.395640045 0.499699265 0.276671946 ... 0.630601108 0.170608789 0.33737433]\n",
      "inverse masked point level mse: [0 0 0 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.44073951\n",
      "Standard batch-level MSE using reduce_sum: 1.44073951\n",
      "Mask length: 23\n",
      "Inverse mask length: 9\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 17.4962654\n",
      "inverse masked sum: 28.6073971\n",
      " \n",
      "masked numerator: 17.4962654\n",
      "unmasked numerator: 28.6073971\n",
      "masked denominator: 23\n",
      "unmasked denominator: 9\n",
      "Masked batch level MSE: 0.7607072\n",
      "Inverse masked batch level MSE: 3.1785996\n",
      " \n",
      "point level mse: [1.26079822 13.4554224 3.36089087 ... 0.191393107 0.459892958 0.792513549]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 1 ... 1 1 1]\n",
      "inverse mask [1 1 0 ... 0 0 0]\n",
      "masked point level mse: [0 0 3.36089087 ... 0.191393107 0.459892958 0.792513549]\n",
      "inverse masked point level mse: [1.26079822 13.4554224 0 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.61581707\n",
      "Standard batch-level MSE using reduce_sum: 0.61581707\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 11.335681\n",
      "inverse masked sum: 8.37046719\n",
      " \n",
      "masked numerator: 11.335681\n",
      "unmasked numerator: 8.37046719\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 0.515258253\n",
      "Inverse masked batch level MSE: 0.837046742\n",
      " \n",
      "point level mse: [0.31845963 1.09428549 0.29736948 ... 0.28538 0.317853 0.478480756]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 1 1 0]\n",
      "inverse mask [0 0 0 ... 0 0 1]\n",
      "masked point level mse: [0.31845963 1.09428549 0.29736948 ... 0.28538 0.317853 0]\n",
      "inverse masked point level mse: [0 0 0 ... 0 0 0.478480756]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.04136145\n",
      "Standard batch-level MSE using reduce_sum: 1.04136145\n",
      "Mask length: 23\n",
      "Inverse mask length: 9\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 10.9386177\n",
      "inverse masked sum: 22.3849487\n",
      " \n",
      "masked numerator: 10.9386177\n",
      "unmasked numerator: 22.3849487\n",
      "masked denominator: 23\n",
      "unmasked denominator: 9\n",
      "Masked batch level MSE: 0.475592077\n",
      "Inverse masked batch level MSE: 2.48721647\n",
      " \n",
      "point level mse: [2.368927 0.413175017 0.391122043 ... 0.516484559 0.498570055 0.427054763]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 1 1 1]\n",
      "inverse mask [0 0 0 ... 0 0 0]\n",
      "masked point level mse: [2.368927 0.413175017 0.391122043 ... 0.516484559 0.498570055 0.427054763]\n",
      "inverse masked point level mse: [0 0 0 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.46768796\n",
      "Standard batch-level MSE using reduce_sum: 1.46768796\n",
      "Mask length: 19\n",
      "Inverse mask length: 13\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 12.5490494\n",
      "inverse masked sum: 34.4169655\n",
      " \n",
      "masked numerator: 12.5490494\n",
      "unmasked numerator: 34.4169655\n",
      "masked denominator: 19\n",
      "unmasked denominator: 13\n",
      "Masked batch level MSE: 0.660476267\n",
      "Inverse masked batch level MSE: 2.64745879\n",
      " \n",
      "point level mse: [0.248192 0.380619019 1.01636577 ... 0.588756382 0.230275691 0.308253497]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 0 ... 0 0 1]\n",
      "inverse mask [0 0 1 ... 1 1 0]\n",
      "masked point level mse: [0.248192 0.380619019 0 ... 0 0 0.308253497]\n",
      "inverse masked point level mse: [0 0 1.01636577 ... 0.588756382 0.230275691 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.26370096\n",
      "Standard batch-level MSE using reduce_sum: 1.26370096\n",
      "Mask length: 21\n",
      "Inverse mask length: 11\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 17.5088634\n",
      "inverse masked sum: 22.9295654\n",
      " \n",
      "masked numerator: 17.5088634\n",
      "unmasked numerator: 22.9295654\n",
      "masked denominator: 21\n",
      "unmasked denominator: 11\n",
      "Masked batch level MSE: 0.833755374\n",
      "Inverse masked batch level MSE: 2.08450603\n",
      " \n",
      "point level mse: [0.838407278 1.8883 2.60904789 ... 1.41117418 0.211281776 3.01099825]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 0 1 0]\n",
      "inverse mask [0 1 0 ... 1 0 1]\n",
      "masked point level mse: [0.838407278 0 2.60904789 ... 0 0.211281776 0]\n",
      "inverse masked point level mse: [0 1.8883 0 ... 1.41117418 0 3.01099825]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.01575208\n",
      "Standard batch-level MSE using reduce_sum: 1.01575208\n",
      "Mask length: 4\n",
      "Inverse mask length: 3\n",
      "n lables in batch: 7\n",
      " \n",
      "masked sum: 3.83537745\n",
      "inverse masked sum: 3.27488685\n",
      " \n",
      "masked numerator: 3.83537745\n",
      "unmasked numerator: 3.27488685\n",
      "masked denominator: 4\n",
      "unmasked denominator: 3\n",
      "Masked batch level MSE: 0.958844364\n",
      "Inverse masked batch level MSE: 1.09162891\n",
      " \n",
      "point level mse: [0.39236173 1.18019 0.610106647 ... 2.61038756 0.211939678 1.48459029]\n",
      " \n",
      "mse shape: TensorShape([7])\n",
      "mask shape: TensorShape([7])\n",
      "inverse mask shape: TensorShape([7])\n",
      "mask [1 0 0 ... 1 1 0]\n",
      "inverse mask [0 1 1 ... 0 0 1]\n",
      "masked point level mse: [0.39236173 0 0 ... 2.61038756 0.211939678 0]\n",
      "inverse masked point level mse: [0 1.18019 0.610106647 ... 0 0 1.48459029]\n",
      "Epoch 7 completed with final batch loss: [0.39236173 0.         0.         0.6206886  2.6103876  0.21193968\n",
      " 0.        ]\n",
      "Epoch 8/10\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.47052169\n",
      "Standard batch-level MSE using reduce_sum: 1.47052169\n",
      "Mask length: 19\n",
      "Inverse mask length: 13\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 11.9530306\n",
      "inverse masked sum: 35.1036682\n",
      " \n",
      "masked numerator: 11.9530306\n",
      "unmasked numerator: 35.1036682\n",
      "masked denominator: 19\n",
      "unmasked denominator: 13\n",
      "Masked batch level MSE: 0.629106879\n",
      "Inverse masked batch level MSE: 2.7002821\n",
      " \n",
      "point level mse: [1.44712234 0.646564245 0.358454049 ... 0.254585028 0.39561832 0.299600154]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 1 1 1]\n",
      "inverse mask [1 0 0 ... 0 0 0]\n",
      "masked point level mse: [0 0.646564245 0.358454049 ... 0.254585028 0.39561832 0.299600154]\n",
      "inverse masked point level mse: [1.44712234 0 0 ... 0 0 0]\n",
      "Step 0: Loss = [0.         0.64656425 0.35845405 0.82678795 0.30086264 0.\n",
      " 0.         0.         0.46523184 0.         0.22649503 0.15554386\n",
      " 0.         0.2989237  0.83138543 0.         0.         1.089368\n",
      " 0.         0.         2.3660305  0.         0.         0.51199317\n",
      " 0.         0.43016148 0.3857356  0.57362735 1.5360622  0.25458503\n",
      " 0.39561832 0.29960015]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.14384007\n",
      "Standard batch-level MSE using reduce_sum: 1.14384007\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 11.9375858\n",
      "inverse masked sum: 24.6652985\n",
      " \n",
      "masked numerator: 11.9375858\n",
      "unmasked numerator: 24.6652985\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 0.542617559\n",
      "Inverse masked batch level MSE: 2.46652985\n",
      " \n",
      "point level mse: [0.31674698 4.96460581 2.597224 ... 5.67292786 0.255966395 0.223011911]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 0 1 1]\n",
      "inverse mask [0 1 0 ... 1 0 0]\n",
      "masked point level mse: [0.31674698 0 2.597224 ... 0 0.255966395 0.223011911]\n",
      "inverse masked point level mse: [0 4.96460581 0 ... 5.67292786 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 2.09521961\n",
      "Standard batch-level MSE using reduce_sum: 2.09521961\n",
      "Mask length: 16\n",
      "Inverse mask length: 16\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 8.08605194\n",
      "inverse masked sum: 58.9609756\n",
      " \n",
      "masked numerator: 8.08605194\n",
      "unmasked numerator: 58.9609756\n",
      "masked denominator: 16\n",
      "unmasked denominator: 16\n",
      "Masked batch level MSE: 0.505378246\n",
      "Inverse masked batch level MSE: 3.68506098\n",
      " \n",
      "point level mse: [2.56898236 2.73338723 1.56684446 ... 0.253502309 2.56432724 0.385694623]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 0 ... 1 0 1]\n",
      "inverse mask [0 1 1 ... 0 1 0]\n",
      "masked point level mse: [2.56898236 0 0 ... 0.253502309 0 0.385694623]\n",
      "inverse masked point level mse: [0 2.73338723 1.56684446 ... 0 2.56432724 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.24180388\n",
      "Standard batch-level MSE using reduce_sum: 1.24180388\n",
      "Mask length: 21\n",
      "Inverse mask length: 11\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 21.037632\n",
      "inverse masked sum: 18.7000923\n",
      " \n",
      "masked numerator: 21.037632\n",
      "unmasked numerator: 18.7000923\n",
      "masked denominator: 21\n",
      "unmasked denominator: 11\n",
      "Masked batch level MSE: 1.00179195\n",
      "Inverse masked batch level MSE: 1.70000839\n",
      " \n",
      "point level mse: [0.330823511 0.2934919 1.42638695 ... 1.46967041 0.525620878 3.03956509]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 0 ... 0 1 0]\n",
      "inverse mask [0 0 1 ... 1 0 1]\n",
      "masked point level mse: [0.330823511 0.2934919 0 ... 0 0.525620878 0]\n",
      "inverse masked point level mse: [0 0 1.42638695 ... 1.46967041 0 3.03956509]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.21449304\n",
      "Standard batch-level MSE using reduce_sum: 1.21449304\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 12.8426399\n",
      "inverse masked sum: 26.0211372\n",
      " \n",
      "masked numerator: 12.8426399\n",
      "unmasked numerator: 26.0211372\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 0.583756387\n",
      "Inverse masked batch level MSE: 2.60211372\n",
      " \n",
      "point level mse: [0.0686062202 3.1113348 3.71109509 ... 0.941050887 3.51407981 0.44254294]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 0 ... 1 0 1]\n",
      "inverse mask [0 1 1 ... 0 1 0]\n",
      "masked point level mse: [0.0686062202 0 0 ... 0.941050887 0 0.44254294]\n",
      "inverse masked point level mse: [0 3.1113348 3.71109509 ... 0 3.51407981 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.18598652\n",
      "Standard batch-level MSE using reduce_sum: 1.18598652\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 14.6963787\n",
      "inverse masked sum: 23.2551918\n",
      " \n",
      "masked numerator: 14.6963787\n",
      "unmasked numerator: 23.2551918\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 0.668017209\n",
      "Inverse masked batch level MSE: 2.32551908\n",
      " \n",
      "point level mse: [0.301767528 0.450229645 1.24728465 ... 0.945415437 0.280033857 0.606754184]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 1 1 0]\n",
      "inverse mask [0 0 0 ... 0 0 1]\n",
      "masked point level mse: [0.301767528 0.450229645 1.24728465 ... 0.945415437 0.280033857 0]\n",
      "inverse masked point level mse: [0 0 0 ... 0 0 0.606754184]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.84617269\n",
      "Standard batch-level MSE using reduce_sum: 0.84617269\n",
      "Mask length: 24\n",
      "Inverse mask length: 8\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 12.6209831\n",
      "inverse masked sum: 14.456543\n",
      " \n",
      "masked numerator: 12.6209831\n",
      "unmasked numerator: 14.456543\n",
      "masked denominator: 24\n",
      "unmasked denominator: 8\n",
      "Masked batch level MSE: 0.525874317\n",
      "Inverse masked batch level MSE: 1.80706787\n",
      " \n",
      "point level mse: [0.306962222 0.555263698 0.361558646 ... 4.51591778 0.480774581 0.454743326]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 0 1 0]\n",
      "inverse mask [0 0 0 ... 1 0 1]\n",
      "masked point level mse: [0.306962222 0.555263698 0.361558646 ... 0 0.480774581 0]\n",
      "inverse masked point level mse: [0 0 0 ... 4.51591778 0 0.454743326]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.44204187\n",
      "Standard batch-level MSE using reduce_sum: 1.44204187\n",
      "Mask length: 18\n",
      "Inverse mask length: 14\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 15.4780521\n",
      "inverse masked sum: 30.6672897\n",
      " \n",
      "masked numerator: 15.4780521\n",
      "unmasked numerator: 30.6672897\n",
      "masked denominator: 18\n",
      "unmasked denominator: 14\n",
      "Masked batch level MSE: 0.859891772\n",
      "Inverse masked batch level MSE: 2.19052076\n",
      " \n",
      "point level mse: [0.877012134 0.384781748 0.141196668 ... 1.19010162 0.511927307 2.54541063]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 0 1 1]\n",
      "inverse mask [0 0 0 ... 1 0 0]\n",
      "masked point level mse: [0.877012134 0.384781748 0.141196668 ... 0 0.511927307 2.54541063]\n",
      "inverse masked point level mse: [0 0 0 ... 1.19010162 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.840650082\n",
      "Standard batch-level MSE using reduce_sum: 0.840650082\n",
      "Mask length: 17\n",
      "Inverse mask length: 15\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 10.2880621\n",
      "inverse masked sum: 16.6127434\n",
      " \n",
      "masked numerator: 10.2880621\n",
      "unmasked numerator: 16.6127434\n",
      "masked denominator: 17\n",
      "unmasked denominator: 15\n",
      "Masked batch level MSE: 0.605180144\n",
      "Inverse masked batch level MSE: 1.10751617\n",
      " \n",
      "point level mse: [0.368576705 1.78486371 0.28691566 ... 0.178973109 0.568866074 0.752941906]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 1 ... 1 0 0]\n",
      "inverse mask [1 1 0 ... 0 1 1]\n",
      "masked point level mse: [0 0 0.28691566 ... 0.178973109 0 0]\n",
      "inverse masked point level mse: [0.368576705 1.78486371 0 ... 0 0.568866074 0.752941906]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.11865044\n",
      "Standard batch-level MSE using reduce_sum: 1.11865044\n",
      "Mask length: 20\n",
      "Inverse mask length: 12\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 10.9998055\n",
      "inverse masked sum: 24.7970085\n",
      " \n",
      "masked numerator: 10.9998055\n",
      "unmasked numerator: 24.7970085\n",
      "masked denominator: 20\n",
      "unmasked denominator: 12\n",
      "Masked batch level MSE: 0.549990296\n",
      "Inverse masked batch level MSE: 2.06641746\n",
      " \n",
      "point level mse: [0.563763618 1.93144476 2.16072607 ... 0.903161705 0.293639481 0.460536659]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 0 ... 1 1 1]\n",
      "inverse mask [0 1 1 ... 0 0 0]\n",
      "masked point level mse: [0.563763618 0 0 ... 0.903161705 0.293639481 0.460536659]\n",
      "inverse masked point level mse: [0 1.93144476 2.16072607 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.813897312\n",
      "Standard batch-level MSE using reduce_sum: 0.813897312\n",
      "Mask length: 20\n",
      "Inverse mask length: 12\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 8.59537125\n",
      "inverse masked sum: 17.4493427\n",
      " \n",
      "masked numerator: 8.59537125\n",
      "unmasked numerator: 17.4493427\n",
      "masked denominator: 20\n",
      "unmasked denominator: 12\n",
      "Masked batch level MSE: 0.429768562\n",
      "Inverse masked batch level MSE: 1.45411193\n",
      " \n",
      "point level mse: [0.825099587 0.992656887 0.807330191 ... 0.300151885 0.687508464 1.88288581]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 0 ... 1 0 0]\n",
      "inverse mask [1 0 1 ... 0 1 1]\n",
      "masked point level mse: [0 0.992656887 0 ... 0.300151885 0 0]\n",
      "inverse masked point level mse: [0.825099587 0 0.807330191 ... 0 0.687508464 1.88288581]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.88201296\n",
      "Standard batch-level MSE using reduce_sum: 1.88201296\n",
      "Mask length: 18\n",
      "Inverse mask length: 14\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 7.98466873\n",
      "inverse masked sum: 52.2397461\n",
      " \n",
      "masked numerator: 7.98466873\n",
      "unmasked numerator: 52.2397461\n",
      "masked denominator: 18\n",
      "unmasked denominator: 14\n",
      "Masked batch level MSE: 0.443592697\n",
      "Inverse masked batch level MSE: 3.7314105\n",
      " \n",
      "point level mse: [1.24450958 0.293174177 0.765528321 ... 0.432647973 2.58932137 0.214380339]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 1 ... 0 0 1]\n",
      "inverse mask [1 1 0 ... 1 1 0]\n",
      "masked point level mse: [0 0 0.765528321 ... 0 0 0.214380339]\n",
      "inverse masked point level mse: [1.24450958 0.293174177 0 ... 0.432647973 2.58932137 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.23044312\n",
      "Standard batch-level MSE using reduce_sum: 1.23044312\n",
      "Mask length: 21\n",
      "Inverse mask length: 11\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 9.66780472\n",
      "inverse masked sum: 29.7063751\n",
      " \n",
      "masked numerator: 9.66780472\n",
      "unmasked numerator: 29.7063751\n",
      "masked denominator: 21\n",
      "unmasked denominator: 11\n",
      "Masked batch level MSE: 0.460371643\n",
      "Inverse masked batch level MSE: 2.70057964\n",
      " \n",
      "point level mse: [0.546650469 2.17264295 0.347125 ... 1.09166408 0.29670319 0.53778857]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 1 1 1]\n",
      "inverse mask [0 1 0 ... 0 0 0]\n",
      "masked point level mse: [0.546650469 0 0.347125 ... 1.09166408 0.29670319 0.53778857]\n",
      "inverse masked point level mse: [0 2.17264295 0 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.714213789\n",
      "Standard batch-level MSE using reduce_sum: 0.714213789\n",
      "Mask length: 21\n",
      "Inverse mask length: 11\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 8.54986191\n",
      "inverse masked sum: 14.3049803\n",
      " \n",
      "masked numerator: 8.54986191\n",
      "unmasked numerator: 14.3049803\n",
      "masked denominator: 21\n",
      "unmasked denominator: 11\n",
      "Masked batch level MSE: 0.407136291\n",
      "Inverse masked batch level MSE: 1.30045271\n",
      " \n",
      "point level mse: [0.192433149 1.46587718 0.423795 ... 0.301784575 0.392341167 0.247852847]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 1 1 1]\n",
      "inverse mask [0 1 0 ... 0 0 0]\n",
      "masked point level mse: [0.192433149 0 0.423795 ... 0.301784575 0.392341167 0.247852847]\n",
      "inverse masked point level mse: [0 1.46587718 0 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.01618135\n",
      "Standard batch-level MSE using reduce_sum: 1.01618135\n",
      "Mask length: 5\n",
      "Inverse mask length: 2\n",
      "n lables in batch: 7\n",
      " \n",
      "masked sum: 2.58039355\n",
      "inverse masked sum: 4.53287601\n",
      " \n",
      "masked numerator: 2.58039355\n",
      "unmasked numerator: 4.53287601\n",
      "masked denominator: 5\n",
      "unmasked denominator: 2\n",
      "Masked batch level MSE: 0.516078711\n",
      "Inverse masked batch level MSE: 2.26643801\n",
      " \n",
      "point level mse: [0.396244198 0.192045465 2.05485439 ... 0.31780225 1.05036628 0.623935342]\n",
      " \n",
      "mse shape: TensorShape([7])\n",
      "mask shape: TensorShape([7])\n",
      "inverse mask shape: TensorShape([7])\n",
      "mask [1 1 0 ... 1 1 1]\n",
      "inverse mask [0 0 1 ... 0 0 0]\n",
      "masked point level mse: [0.396244198 0.192045465 0 ... 0.31780225 1.05036628 0.623935342]\n",
      "inverse masked point level mse: [0 0 2.05485439 ... 0 0 0]\n",
      "Epoch 8 completed with final batch loss: [0.3962442  0.19204547 0.         0.         0.31780225 1.0503663\n",
      " 0.62393534]\n",
      "Epoch 9/10\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.29627824\n",
      "Standard batch-level MSE using reduce_sum: 1.29627824\n",
      "Mask length: 18\n",
      "Inverse mask length: 14\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 9.02144\n",
      "inverse masked sum: 32.4594612\n",
      " \n",
      "masked numerator: 9.02144\n",
      "unmasked numerator: 32.4594612\n",
      "masked denominator: 18\n",
      "unmasked denominator: 14\n",
      "Masked batch level MSE: 0.50119108\n",
      "Inverse masked batch level MSE: 2.31853294\n",
      " \n",
      "point level mse: [0.362742633 0.385685444 0.54840076 ... 0.175548449 0.756297708 0.120574027]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 1 0 1]\n",
      "inverse mask [0 0 0 ... 0 1 0]\n",
      "masked point level mse: [0.362742633 0.385685444 0.54840076 ... 0.175548449 0 0.120574027]\n",
      "inverse masked point level mse: [0 0 0 ... 0 0.756297708 0]\n",
      "Step 0: Loss = [0.36274263 0.38568544 0.54840076 0.55260086 0.         0.12370931\n",
      " 0.         0.4684594  0.34228283 0.         0.80277765 0.365171\n",
      " 1.3765991  0.         0.         0.27895775 0.         0.56258523\n",
      " 0.         0.         0.         0.17548189 0.         0.\n",
      " 0.         0.         0.17491962 2.0955548  0.109389   0.17554845\n",
      " 0.         0.12057403]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 2.17575312\n",
      "Standard batch-level MSE using reduce_sum: 2.17575312\n",
      "Mask length: 19\n",
      "Inverse mask length: 13\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 12.6116495\n",
      "inverse masked sum: 57.0124435\n",
      " \n",
      "masked numerator: 12.6116495\n",
      "unmasked numerator: 57.0124435\n",
      "masked denominator: 19\n",
      "unmasked denominator: 13\n",
      "Masked batch level MSE: 0.663771033\n",
      "Inverse masked batch level MSE: 4.38557243\n",
      " \n",
      "point level mse: [2.97540545 0.326236516 0.418932348 ... 0.743943453 4.79337072 0.870848835]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 1 0 1]\n",
      "inverse mask [0 1 0 ... 0 1 0]\n",
      "masked point level mse: [2.97540545 0 0.418932348 ... 0.743943453 0 0.870848835]\n",
      "inverse masked point level mse: [0 0.326236516 0 ... 0 4.79337072 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.771437049\n",
      "Standard batch-level MSE using reduce_sum: 0.771437049\n",
      "Mask length: 24\n",
      "Inverse mask length: 8\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 8.68663883\n",
      "inverse masked sum: 15.9993458\n",
      " \n",
      "masked numerator: 8.68663883\n",
      "unmasked numerator: 15.9993458\n",
      "masked denominator: 24\n",
      "unmasked denominator: 8\n",
      "Masked batch level MSE: 0.361943275\n",
      "Inverse masked batch level MSE: 1.99991822\n",
      " \n",
      "point level mse: [0.22832264 0.194014892 0.32473436 ... 0.349280417 2.46950078 1.78804648]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 1 0 0]\n",
      "inverse mask [0 0 0 ... 0 1 1]\n",
      "masked point level mse: [0.22832264 0.194014892 0.32473436 ... 0.349280417 0 0]\n",
      "inverse masked point level mse: [0 0 0 ... 0 2.46950078 1.78804648]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.777037263\n",
      "Standard batch-level MSE using reduce_sum: 0.777037263\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 11.0777483\n",
      "inverse masked sum: 13.7874451\n",
      " \n",
      "masked numerator: 11.0777483\n",
      "unmasked numerator: 13.7874451\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 0.503534\n",
      "Inverse masked batch level MSE: 1.37874448\n",
      " \n",
      "point level mse: [1.19636977 0.399369985 0.317563742 ... 0.462140769 0.643526375 0.368210971]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 1 1 1]\n",
      "inverse mask [1 0 0 ... 0 0 0]\n",
      "masked point level mse: [0 0.399369985 0.317563742 ... 0.462140769 0.643526375 0.368210971]\n",
      "inverse masked point level mse: [1.19636977 0 0 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.986535072\n",
      "Standard batch-level MSE using reduce_sum: 0.986535072\n",
      "Mask length: 21\n",
      "Inverse mask length: 11\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 10.1282024\n",
      "inverse masked sum: 21.440918\n",
      " \n",
      "masked numerator: 10.1282024\n",
      "unmasked numerator: 21.440918\n",
      "masked denominator: 21\n",
      "unmasked denominator: 11\n",
      "Masked batch level MSE: 0.482295364\n",
      "Inverse masked batch level MSE: 1.9491744\n",
      " \n",
      "point level mse: [2.66071582 0.420854062 1.00347495 ... 1.45169461 0.263657153 0.422611237]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 0 1 1]\n",
      "inverse mask [1 0 0 ... 1 0 0]\n",
      "masked point level mse: [0 0.420854062 1.00347495 ... 0 0.263657153 0.422611237]\n",
      "inverse masked point level mse: [2.66071582 0 0 ... 1.45169461 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.61621857\n",
      "Standard batch-level MSE using reduce_sum: 1.61621857\n",
      "Mask length: 23\n",
      "Inverse mask length: 9\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 31.9121857\n",
      "inverse masked sum: 19.8068085\n",
      " \n",
      "masked numerator: 31.9121857\n",
      "unmasked numerator: 19.8068085\n",
      "masked denominator: 23\n",
      "unmasked denominator: 9\n",
      "Masked batch level MSE: 1.38748634\n",
      "Inverse masked batch level MSE: 2.20075655\n",
      " \n",
      "point level mse: [1.85884225 2.95361376 0.815862119 ... 1.80652881 0.689777732 0.324392885]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 0 ... 1 1 1]\n",
      "inverse mask [1 1 1 ... 0 0 0]\n",
      "masked point level mse: [0 0 0 ... 1.80652881 0.689777732 0.324392885]\n",
      "inverse masked point level mse: [1.85884225 2.95361376 0.815862119 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.5741657\n",
      "Standard batch-level MSE using reduce_sum: 1.5741657\n",
      "Mask length: 20\n",
      "Inverse mask length: 12\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 7.75622368\n",
      "inverse masked sum: 42.6170769\n",
      " \n",
      "masked numerator: 7.75622368\n",
      "unmasked numerator: 42.6170769\n",
      "masked denominator: 20\n",
      "unmasked denominator: 12\n",
      "Masked batch level MSE: 0.387811184\n",
      "Inverse masked batch level MSE: 3.55142307\n",
      " \n",
      "point level mse: [1.4474237 0.618095517 0.946677744 ... 0.686429083 0.414881736 0.403474212]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 0 1 1]\n",
      "inverse mask [1 0 0 ... 1 0 0]\n",
      "masked point level mse: [0 0.618095517 0.946677744 ... 0 0.414881736 0.403474212]\n",
      "inverse masked point level mse: [1.4474237 0 0 ... 0.686429083 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.13172\n",
      "Standard batch-level MSE using reduce_sum: 1.13172\n",
      "Mask length: 17\n",
      "Inverse mask length: 15\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 6.82470894\n",
      "inverse masked sum: 29.3903313\n",
      " \n",
      "masked numerator: 6.82470894\n",
      "unmasked numerator: 29.3903313\n",
      "masked denominator: 17\n",
      "unmasked denominator: 15\n",
      "Masked batch level MSE: 0.401453465\n",
      "Inverse masked batch level MSE: 1.95935547\n",
      " \n",
      "point level mse: [0.436613739 4.37440825 0.549135387 ... 0.232624054 2.09860349 0.22166039]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 1 0 1]\n",
      "inverse mask [0 1 0 ... 0 1 0]\n",
      "masked point level mse: [0.436613739 0 0.549135387 ... 0.232624054 0 0.22166039]\n",
      "inverse masked point level mse: [0 4.37440825 0 ... 0 2.09860349 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 2.05318\n",
      "Standard batch-level MSE using reduce_sum: 2.05318\n",
      "Mask length: 14\n",
      "Inverse mask length: 18\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 6.80390835\n",
      "inverse masked sum: 58.8978539\n",
      " \n",
      "masked numerator: 6.80390835\n",
      "unmasked numerator: 58.8978539\n",
      "masked denominator: 14\n",
      "unmasked denominator: 18\n",
      "Masked batch level MSE: 0.485993445\n",
      "Inverse masked batch level MSE: 3.27210307\n",
      " \n",
      "point level mse: [0.290955067 1.77042055 2.50796771 ... 0.296531796 1.85768402 1.27690697]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 1 ... 1 0 0]\n",
      "inverse mask [1 1 0 ... 0 1 1]\n",
      "masked point level mse: [0 0 2.50796771 ... 0.296531796 0 0]\n",
      "inverse masked point level mse: [0.290955067 1.77042055 0 ... 0 1.85768402 1.27690697]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.873629928\n",
      "Standard batch-level MSE using reduce_sum: 0.873629928\n",
      "Mask length: 20\n",
      "Inverse mask length: 12\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 7.28927755\n",
      "inverse masked sum: 20.6668777\n",
      " \n",
      "masked numerator: 7.28927755\n",
      "unmasked numerator: 20.6668777\n",
      "masked denominator: 20\n",
      "unmasked denominator: 12\n",
      "Masked batch level MSE: 0.364463866\n",
      "Inverse masked batch level MSE: 1.72223985\n",
      " \n",
      "point level mse: [0.264538854 1.23796332 0.387658417 ... 0.139586791 0.191145688 0.133750901]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 1 1 1]\n",
      "inverse mask [0 1 0 ... 0 0 0]\n",
      "masked point level mse: [0.264538854 0 0.387658417 ... 0.139586791 0.191145688 0.133750901]\n",
      "inverse masked point level mse: [0 1.23796332 0 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.788638711\n",
      "Standard batch-level MSE using reduce_sum: 0.788638711\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 11.5883312\n",
      "inverse masked sum: 13.6481075\n",
      " \n",
      "masked numerator: 11.5883312\n",
      "unmasked numerator: 13.6481075\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 0.526742339\n",
      "Inverse masked batch level MSE: 1.36481071\n",
      " \n",
      "point level mse: [0.443625391 1.50542128 0.412885368 ... 0.509347796 0.474794209 5.9029603]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 0 1 0]\n",
      "inverse mask [1 0 0 ... 1 0 1]\n",
      "masked point level mse: [0 1.50542128 0.412885368 ... 0 0.474794209 0]\n",
      "inverse masked point level mse: [0.443625391 0 0 ... 0.509347796 0 5.9029603]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.08986211\n",
      "Standard batch-level MSE using reduce_sum: 1.08986211\n",
      "Mask length: 21\n",
      "Inverse mask length: 11\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 12.7723131\n",
      "inverse masked sum: 22.1032753\n",
      " \n",
      "masked numerator: 12.7723131\n",
      "unmasked numerator: 22.1032753\n",
      "masked denominator: 21\n",
      "unmasked denominator: 11\n",
      "Masked batch level MSE: 0.608205378\n",
      "Inverse masked batch level MSE: 2.00938869\n",
      " \n",
      "point level mse: [1.25944567 1.05979359 2.09812689 ... 0.694563866 0.419250339 2.94423461]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 0 ... 0 1 1]\n",
      "inverse mask [1 1 1 ... 1 0 0]\n",
      "masked point level mse: [0 0 0 ... 0 0.419250339 2.94423461]\n",
      "inverse masked point level mse: [1.25944567 1.05979359 2.09812689 ... 0.694563866 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.95087862\n",
      "Standard batch-level MSE using reduce_sum: 0.95087862\n",
      "Mask length: 18\n",
      "Inverse mask length: 14\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 8.69132233\n",
      "inverse masked sum: 21.7367916\n",
      " \n",
      "masked numerator: 8.69132233\n",
      "unmasked numerator: 21.7367916\n",
      "masked denominator: 18\n",
      "unmasked denominator: 14\n",
      "Masked batch level MSE: 0.482851237\n",
      "Inverse masked batch level MSE: 1.55262792\n",
      " \n",
      "point level mse: [0.329131633 0.631169379 6.41886806 ... 0.591572106 1.50029302 0.11556676]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 0 ... 1 0 1]\n",
      "inverse mask [0 0 1 ... 0 1 0]\n",
      "masked point level mse: [0.329131633 0.631169379 0 ... 0.591572106 0 0.11556676]\n",
      "inverse masked point level mse: [0 0 6.41886806 ... 0 1.50029302 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.712057\n",
      "Standard batch-level MSE using reduce_sum: 0.712057\n",
      "Mask length: 24\n",
      "Inverse mask length: 8\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 12.3993006\n",
      "inverse masked sum: 10.3865242\n",
      " \n",
      "masked numerator: 12.3993006\n",
      "unmasked numerator: 10.3865242\n",
      "masked denominator: 24\n",
      "unmasked denominator: 8\n",
      "Masked batch level MSE: 0.516637504\n",
      "Inverse masked batch level MSE: 1.29831553\n",
      " \n",
      "point level mse: [1.24514842 0.370645523 0.494252622 ... 0.509388626 0.154945225 0.190172762]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 1 1 1]\n",
      "inverse mask [1 0 0 ... 0 0 0]\n",
      "masked point level mse: [0 0.370645523 0.494252622 ... 0.509388626 0.154945225 0.190172762]\n",
      "inverse masked point level mse: [1.24514842 0 0 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.04315233\n",
      "Standard batch-level MSE using reduce_sum: 1.04315233\n",
      "Mask length: 3\n",
      "Inverse mask length: 4\n",
      "n lables in batch: 7\n",
      " \n",
      "masked sum: 1.78050506\n",
      "inverse masked sum: 5.52156162\n",
      " \n",
      "masked numerator: 1.78050506\n",
      "unmasked numerator: 5.52156162\n",
      "masked denominator: 3\n",
      "unmasked denominator: 4\n",
      "Masked batch level MSE: 0.593501687\n",
      "Inverse masked batch level MSE: 1.38039041\n",
      " \n",
      "point level mse: [0.2553325 0.316761941 1.58973467 ... 0.538158774 1.4514854 0.987013817]\n",
      " \n",
      "mse shape: TensorShape([7])\n",
      "mask shape: TensorShape([7])\n",
      "inverse mask shape: TensorShape([7])\n",
      "mask [1 0 0 ... 1 0 1]\n",
      "inverse mask [0 1 1 ... 0 1 0]\n",
      "masked point level mse: [0.2553325 0 0 ... 0.538158774 0 0.987013817]\n",
      "inverse masked point level mse: [0 0.316761941 1.58973467 ... 0 1.4514854 0]\n",
      "Epoch 9 completed with final batch loss: [0.2553325 0.        0.        0.        0.5381588 0.        0.9870138]\n",
      "Epoch 10/10\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.93316126\n",
      "Standard batch-level MSE using reduce_sum: 1.93316126\n",
      "Mask length: 18\n",
      "Inverse mask length: 14\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 12.2718191\n",
      "inverse masked sum: 49.5893402\n",
      " \n",
      "masked numerator: 12.2718191\n",
      "unmasked numerator: 49.5893402\n",
      "masked denominator: 18\n",
      "unmasked denominator: 14\n",
      "Masked batch level MSE: 0.681767702\n",
      "Inverse masked batch level MSE: 3.54209566\n",
      " \n",
      "point level mse: [1.69819951 0.357869536 2.93245697 ... 0.371990889 0.102050133 0.479967624]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 1 1 1]\n",
      "inverse mask [1 0 0 ... 0 0 0]\n",
      "masked point level mse: [0 0.357869536 2.93245697 ... 0.371990889 0.102050133 0.479967624]\n",
      "inverse masked point level mse: [1.69819951 0 0 ... 0 0 0]\n",
      "Step 0: Loss = [0.         0.35786954 2.932457   0.         0.3073193  0.\n",
      " 0.         0.36289012 0.         1.5040194  0.2689003  1.8943901\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.27219453 0.         0.66842186 0.         0.         0.38009226\n",
      " 0.4738326  0.31550953 0.17615324 0.77918524 0.62457526 0.3719909\n",
      " 0.10205013 0.47996762]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.0607729\n",
      "Standard batch-level MSE using reduce_sum: 1.0607729\n",
      "Mask length: 19\n",
      "Inverse mask length: 13\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 8.69718075\n",
      "inverse masked sum: 25.2475529\n",
      " \n",
      "masked numerator: 8.69718075\n",
      "unmasked numerator: 25.2475529\n",
      "masked denominator: 19\n",
      "unmasked denominator: 13\n",
      "Masked batch level MSE: 0.457746357\n",
      "Inverse masked batch level MSE: 1.94211948\n",
      " \n",
      "point level mse: [0.384825826 2.48176837 0.737335086 ... 0.699877799 0.997273505 0.794288695]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 0 ... 0 0 0]\n",
      "inverse mask [0 0 1 ... 1 1 1]\n",
      "masked point level mse: [0.384825826 2.48176837 0 ... 0 0 0]\n",
      "inverse masked point level mse: [0 0 0.737335086 ... 0.699877799 0.997273505 0.794288695]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.706337452\n",
      "Standard batch-level MSE using reduce_sum: 0.706337452\n",
      "Mask length: 21\n",
      "Inverse mask length: 11\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 7.46571159\n",
      "inverse masked sum: 15.1370869\n",
      " \n",
      "masked numerator: 7.46571159\n",
      "unmasked numerator: 15.1370869\n",
      "masked denominator: 21\n",
      "unmasked denominator: 11\n",
      "Masked batch level MSE: 0.355510086\n",
      "Inverse masked batch level MSE: 1.37609875\n",
      " \n",
      "point level mse: [0.208931342 0.713512599 0.396325946 ... 0.278025359 0.51745671 1.6938659]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 1 1 0]\n",
      "inverse mask [0 0 0 ... 0 0 1]\n",
      "masked point level mse: [0.208931342 0.713512599 0.396325946 ... 0.278025359 0.51745671 0]\n",
      "inverse masked point level mse: [0 0 0 ... 0 0 1.6938659]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.70759511\n",
      "Standard batch-level MSE using reduce_sum: 1.70759511\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 10.1878872\n",
      "inverse masked sum: 44.4551582\n",
      " \n",
      "masked numerator: 10.1878872\n",
      "unmasked numerator: 44.4551582\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 0.463085771\n",
      "Inverse masked batch level MSE: 4.44551563\n",
      " \n",
      "point level mse: [0.188611209 0.217449605 1.74472857 ... 0.413052082 0.585318208 0.319922328]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 0 ... 1 1 1]\n",
      "inverse mask [0 0 1 ... 0 0 0]\n",
      "masked point level mse: [0.188611209 0.217449605 0 ... 0.413052082 0.585318208 0.319922328]\n",
      "inverse masked point level mse: [0 0 1.74472857 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.631367266\n",
      "Standard batch-level MSE using reduce_sum: 0.631367266\n",
      "Mask length: 22\n",
      "Inverse mask length: 10\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 8.67340565\n",
      "inverse masked sum: 11.5303469\n",
      " \n",
      "masked numerator: 8.67340565\n",
      "unmasked numerator: 11.5303469\n",
      "masked denominator: 22\n",
      "unmasked denominator: 10\n",
      "Masked batch level MSE: 0.394245714\n",
      "Inverse masked batch level MSE: 1.15303469\n",
      " \n",
      "point level mse: [0.275615811 0.180558935 0.220230266 ... 0.351521671 2.53036356 0.202029169]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 1 0 1]\n",
      "inverse mask [0 0 0 ... 0 1 0]\n",
      "masked point level mse: [0.275615811 0.180558935 0.220230266 ... 0.351521671 0 0.202029169]\n",
      "inverse masked point level mse: [0 0 0 ... 0 2.53036356 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.721933603\n",
      "Standard batch-level MSE using reduce_sum: 0.721933603\n",
      "Mask length: 18\n",
      "Inverse mask length: 14\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 6.28143787\n",
      "inverse masked sum: 16.8204384\n",
      " \n",
      "masked numerator: 6.28143787\n",
      "unmasked numerator: 16.8204384\n",
      "masked denominator: 18\n",
      "unmasked denominator: 14\n",
      "Masked batch level MSE: 0.348968774\n",
      "Inverse masked batch level MSE: 1.20145988\n",
      " \n",
      "point level mse: [0.251224756 0.722100198 0.285323888 ... 0.516331077 0.268910289 1.05510783]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 1 ... 0 1 0]\n",
      "inverse mask [1 1 0 ... 1 0 1]\n",
      "masked point level mse: [0 0 0.285323888 ... 0 0.268910289 0]\n",
      "inverse masked point level mse: [0.251224756 0.722100198 0 ... 0.516331077 0 1.05510783]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.16274905\n",
      "Standard batch-level MSE using reduce_sum: 1.16274905\n",
      "Mask length: 23\n",
      "Inverse mask length: 9\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 19.831789\n",
      "inverse masked sum: 17.3761787\n",
      " \n",
      "masked numerator: 19.831789\n",
      "unmasked numerator: 17.3761787\n",
      "masked denominator: 23\n",
      "unmasked denominator: 9\n",
      "Masked batch level MSE: 0.862251699\n",
      "Inverse masked batch level MSE: 1.93068647\n",
      " \n",
      "point level mse: [0.304794788 0.837397337 2.154809 ... 0.165075228 0.249182776 0.277203679]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 0 ... 1 1 1]\n",
      "inverse mask [0 1 1 ... 0 0 0]\n",
      "masked point level mse: [0.304794788 0 0 ... 0.165075228 0.249182776 0.277203679]\n",
      "inverse masked point level mse: [0 0.837397337 2.154809 ... 0 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.921822906\n",
      "Standard batch-level MSE using reduce_sum: 0.921822906\n",
      "Mask length: 19\n",
      "Inverse mask length: 13\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 6.08026123\n",
      "inverse masked sum: 23.4180698\n",
      " \n",
      "masked numerator: 6.08026123\n",
      "unmasked numerator: 23.4180698\n",
      "masked denominator: 19\n",
      "unmasked denominator: 13\n",
      "Masked batch level MSE: 0.320013762\n",
      "Inverse masked batch level MSE: 1.80138993\n",
      " \n",
      "point level mse: [0.464520872 0.306942344 0.593840063 ... 0.283522606 0.658023715 0.137237832]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 1 0 1]\n",
      "inverse mask [0 0 0 ... 0 1 0]\n",
      "masked point level mse: [0.464520872 0.306942344 0.593840063 ... 0.283522606 0 0.137237832]\n",
      "inverse masked point level mse: [0 0 0 ... 0 0.658023715 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.49869013\n",
      "Standard batch-level MSE using reduce_sum: 1.49869013\n",
      "Mask length: 19\n",
      "Inverse mask length: 13\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 12.5001278\n",
      "inverse masked sum: 35.457962\n",
      " \n",
      "masked numerator: 12.5001278\n",
      "unmasked numerator: 35.457962\n",
      "masked denominator: 19\n",
      "unmasked denominator: 13\n",
      "Masked batch level MSE: 0.657901466\n",
      "Inverse masked batch level MSE: 2.72753549\n",
      " \n",
      "point level mse: [1.73442197 1.78596604 1.78259456 ... 4.31359816 0.455011517 0.610211253]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 0 ... 0 1 0]\n",
      "inverse mask [1 1 1 ... 1 0 1]\n",
      "masked point level mse: [0 0 0 ... 0 0.455011517 0]\n",
      "inverse masked point level mse: [1.73442197 1.78596604 1.78259456 ... 4.31359816 0 0.610211253]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.19664311\n",
      "Standard batch-level MSE using reduce_sum: 1.19664311\n",
      "Mask length: 23\n",
      "Inverse mask length: 9\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 14.3652325\n",
      "inverse masked sum: 23.9273491\n",
      " \n",
      "masked numerator: 14.3652325\n",
      "unmasked numerator: 23.9273491\n",
      "masked denominator: 23\n",
      "unmasked denominator: 9\n",
      "Masked batch level MSE: 0.624575317\n",
      "Inverse masked batch level MSE: 2.65859437\n",
      " \n",
      "point level mse: [0.341789097 1.22596407 0.206256419 ... 0.298107684 2.56796217 3.39378285]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 1 1 0]\n",
      "inverse mask [0 0 0 ... 0 0 1]\n",
      "masked point level mse: [0.341789097 1.22596407 0.206256419 ... 0.298107684 2.56796217 0]\n",
      "inverse masked point level mse: [0 0 0 ... 0 0 3.39378285]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.981732\n",
      "Standard batch-level MSE using reduce_sum: 0.981732\n",
      "Mask length: 18\n",
      "Inverse mask length: 14\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 7.30247974\n",
      "inverse masked sum: 24.1129436\n",
      " \n",
      "masked numerator: 7.30247974\n",
      "unmasked numerator: 24.1129436\n",
      "masked denominator: 18\n",
      "unmasked denominator: 14\n",
      "Masked batch level MSE: 0.405693322\n",
      "Inverse masked batch level MSE: 1.7223531\n",
      " \n",
      "point level mse: [0.414405912 0.261747509 0.147586122 ... 2.71769643 0.917692304 0.261629611]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 1 1 ... 0 1 1]\n",
      "inverse mask [0 0 0 ... 1 0 0]\n",
      "masked point level mse: [0.414405912 0.261747509 0.147586122 ... 0 0.917692304 0.261629611]\n",
      "inverse masked point level mse: [0 0 0 ... 2.71769643 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.26307178\n",
      "Standard batch-level MSE using reduce_sum: 1.26307178\n",
      "Mask length: 19\n",
      "Inverse mask length: 13\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 10.9910669\n",
      "inverse masked sum: 29.4272308\n",
      " \n",
      "masked numerator: 10.9910669\n",
      "unmasked numerator: 29.4272308\n",
      "masked denominator: 19\n",
      "unmasked denominator: 13\n",
      "Masked batch level MSE: 0.578477204\n",
      "Inverse masked batch level MSE: 2.26363325\n",
      " \n",
      "point level mse: [0.309398681 0.429154247 0.265481979 ... 0.124927215 2.9029696 2.64100742]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 1 1 ... 1 1 0]\n",
      "inverse mask [1 0 0 ... 0 0 1]\n",
      "masked point level mse: [0 0.429154247 0.265481979 ... 0.124927215 2.9029696 0]\n",
      "inverse masked point level mse: [0.309398681 0 0 ... 0 0 2.64100742]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.46363783\n",
      "Standard batch-level MSE using reduce_sum: 1.46363783\n",
      "Mask length: 18\n",
      "Inverse mask length: 14\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 7.02550125\n",
      "inverse masked sum: 39.8109093\n",
      " \n",
      "masked numerator: 7.02550125\n",
      "unmasked numerator: 39.8109093\n",
      "masked denominator: 18\n",
      "unmasked denominator: 14\n",
      "Masked batch level MSE: 0.390305638\n",
      "Inverse masked batch level MSE: 2.84363627\n",
      " \n",
      "point level mse: [0.605850339 1.81151509 0.268590361 ... 1.96642959 0.504190505 0.530014813]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [1 0 1 ... 0 1 1]\n",
      "inverse mask [0 1 0 ... 1 0 0]\n",
      "masked point level mse: [0.605850339 0 0.268590361 ... 0 0.504190505 0.530014813]\n",
      "inverse masked point level mse: [0 1.81151509 0 ... 1.96642959 0 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 1.18699276\n",
      "Standard batch-level MSE using reduce_sum: 1.18699276\n",
      "Mask length: 21\n",
      "Inverse mask length: 11\n",
      "n lables in batch: 32\n",
      " \n",
      "masked sum: 18.2296848\n",
      "inverse masked sum: 19.7540836\n",
      " \n",
      "masked numerator: 18.2296848\n",
      "unmasked numerator: 19.7540836\n",
      "masked denominator: 21\n",
      "unmasked denominator: 11\n",
      "Masked batch level MSE: 0.868080258\n",
      "Inverse masked batch level MSE: 1.79582584\n",
      " \n",
      "point level mse: [0.236557528 1.64028013 1.51650226 ... 2.05976319 2.86295223 3.04908061]\n",
      " \n",
      "mse shape: TensorShape([32])\n",
      "mask shape: TensorShape([32])\n",
      "inverse mask shape: TensorShape([32])\n",
      "mask [0 0 1 ... 1 0 1]\n",
      "inverse mask [1 1 0 ... 0 1 0]\n",
      "masked point level mse: [0 0 1.51650226 ... 2.05976319 0 3.04908061]\n",
      "inverse masked point level mse: [0.236557528 1.64028013 0 ... 0 2.86295223 0]\n",
      "-----------------------------\n",
      "Standard batch-level MSE: 0.505779803\n",
      "Standard batch-level MSE using reduce_sum: 0.505779803\n",
      "Mask length: 6\n",
      "Inverse mask length: 1\n",
      "n lables in batch: 7\n",
      " \n",
      "masked sum: 2.59678268\n",
      "inverse masked sum: 0.943675756\n",
      " \n",
      "masked numerator: 2.59678268\n",
      "unmasked numerator: 0.943675756\n",
      "masked denominator: 6\n",
      "unmasked denominator: 1\n",
      "Masked batch level MSE: 0.432797104\n",
      "Inverse masked batch level MSE: 0.943675756\n",
      " \n",
      "point level mse: [0.615984797 0.943675756 0.606404066 ... 0.178191721 0.276886821 0.405050576]\n",
      " \n",
      "mse shape: TensorShape([7])\n",
      "mask shape: TensorShape([7])\n",
      "inverse mask shape: TensorShape([7])\n",
      "mask [1 0 1 ... 1 1 1]\n",
      "inverse mask [0 1 0 ... 0 0 0]\n",
      "masked point level mse: [0.615984797 0 0.606404066 ... 0.178191721 0.276886821 0.405050576]\n",
      "inverse masked point level mse: [0 0.943675756 0 ... 0 0 0]\n",
      "Epoch 10 completed with final batch loss: [0.6159848  0.         0.60640407 0.5142647  0.17819172 0.27688682\n",
      " 0.40505058]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert X_train and y_train to float32 if not already converted\n",
    "if X_train.dtype != tf.float32:\n",
    "    X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "\n",
    "if y_train.dtype != tf.float32:\n",
    "    y_train = tf.convert_to_tensor(y_train.values.reshape(-1, 1), dtype=tf.float32)\n",
    "\n",
    "# Create a dataset that includes features and labels\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "\n",
    "# Shuffle and batch the dataset\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Define the autoencoder model (make sure it also outputs float32)\n",
    "autoencoder_pos = create_autoencoder(input_dim)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Number of epochs to train\n",
    "num_epochs = 10\n",
    "\n",
    "# Custom training loop to replicate `model.fit`\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    # Iterate over the batches of the dataset\n",
    "    for step, (X_batch, labels_batch) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            X_pred = autoencoder_pos(X_batch, training=True)\n",
    "\n",
    "            # Compute the loss using the custom loss function\n",
    "            loss = custom_loss_pos(X_batch, X_pred, labels_batch)\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(loss, autoencoder_pos.trainable_weights)\n",
    "\n",
    "        # Apply gradients to update the model's weights\n",
    "        optimizer.apply_gradients(zip(gradients, autoencoder_pos.trainable_weights))\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}: Loss = {loss.numpy()}\")\n",
    "\n",
    "    # Print the loss for the epoch (optional, for monitoring)\n",
    "    print(f\"Epoch {epoch+1} completed with final batch loss: {loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 612us/step\n",
      "4/4 [==============================] - 0s 653us/step\n",
      "4/4 [==============================] - 0s 622us/step\n",
      "4/4 [==============================] - 0s 610us/step\n"
     ]
    }
   ],
   "source": [
    "#get feature-level reconstruction errors for positive and negative autoencoder\n",
    "X_test_normal = autoencoder_normal.predict(X_test)\n",
    "X_test_benign = autoencoder_benign.predict(X_test)\n",
    "X_test_malignant = autoencoder_malignant.predict(X_test)\n",
    "X_test_pos = autoencoder_pos.predict(X_test)\n",
    "\n",
    "# get the patient-level (ie point-level) reconstruction errors (ie the scores)\n",
    "scores_normal = np.mean(np.power(X_test - X_test_normal, 2), axis=1)\n",
    "scores_benign = np.mean(np.power(X_test - X_test_benign, 2), axis=1)\n",
    "scores_malignant = np.mean(np.power(X_test - X_test_malignant, 2), axis=1)\n",
    "scores_pos = np.mean(np.power(X_test - X_test_pos, 2), axis=1)\n",
    "\n",
    "# Calculate the rocs and aucs\n",
    "fpr_normal, tpr_normal, _ = roc_curve(y_test, scores_normal)\n",
    "roc_auc_normal = auc(fpr_normal, tpr_normal)\n",
    "fpr_benign, tpr_benign, _ = roc_curve(y_test, scores_benign)\n",
    "roc_auc_benign = auc(fpr_benign, tpr_benign)\n",
    "fpr_malignant, tpr_malignant, _ = roc_curve(y_test, scores_malignant)\n",
    "roc_auc_malignant = auc(fpr_malignant, tpr_malignant)\n",
    "fpr_pos, tpr_pos, _ = roc_curve(y_test, scores_pos)\n",
    "roc_auc_pos = auc(fpr_pos, tpr_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the roc curves\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr_normal, tpr_normal, color='darkorange', lw=lw, label='Normal Autoencoder (area = %0.2f)' % roc_auc_normal)\n",
    "plt.plot(fpr_benign, tpr_benign, color='green', lw=lw, label='Benign Autoencoder (area = %0.2f)' % roc_auc_benign)\n",
    "plt.plot(fpr_malignant, tpr_malignant, color='red', lw=lw, label='Malignant Autoencoder (area = %0.2f)' % roc_auc_malignant)\n",
    "plt.plot(fpr_pos, tpr_pos, color='blue', lw=lw, label='Positive Autoencoder (area = %0.2f)' % roc_auc_pos)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
