{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import roc_curve, auc, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "#ucimlrepo is for the data\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "\n",
    " # Fetch the Breast Cancer Wisconsin dataset from UCI repository\n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)\n",
    "\n",
    "# Get the features (X) and targets (y) as pandas DataFrames\n",
    "X = breast_cancer_wisconsin_diagnostic.data.features\n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets\n",
    "\n",
    "# Print columns to verify the structure\n",
    "#print(\"Feature columns:\\n\", X.columns)\n",
    "#print(\"Target columns:\\n\", y.columns)\n",
    "\n",
    "y = y.iloc[:, 0]  # extract the Series\n",
    "\n",
    "# Map the diagnosis labels: 1 for malignant ('M'), 0 for benign ('B')\n",
    "y = y.map({'M': 1, 'B': 0})  # Adjust this mapping as per the dataset's structure\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the input dimension\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# take a subset of the training data that only has benign samples\n",
    "X_train_benign = X_train[y_train == 0]\n",
    "\n",
    "# input dimension for the benign model\n",
    "input_dim_benign = X_train_benign.shape[1]\n",
    "\n",
    "#take a subset of the training data that only has malignant samples\n",
    "X_train_malignant = X_train[y_train == 1]\n",
    "\n",
    "# input dimension for the malignant model\n",
    "input_dim_malignant = X_train_malignant.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to define the shared model arhitecture \n",
    "def create_autoencoder(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(14, activation='relu', input_shape=(input_dim,)),\n",
    "        Dense(7, activation='relu'),\n",
    "        Dense(14, activation='relu'),\n",
    "        Dense(input_dim, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create the standard  autoencoder models\n",
    "autoencoder_normal = create_autoencoder(input_dim)\n",
    "autoencoder_benign = create_autoencoder(input_dim_benign)\n",
    "autoencoder_malignant = create_autoencoder(input_dim_malignant)\n",
    "\n",
    "\n",
    "# Compile the standard models witht the adam optimizer\n",
    "autoencoder_normal.compile(optimizer='adam', loss='mean_squared_error')\n",
    "autoencoder_benign.compile(optimizer='adam', loss='mean_squared_error')\n",
    "autoencoder_malignant.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "# train the standard models\n",
    "autoencoder_normal.fit(X_train, X_train, epochs=30, batch_size=32,verbose=1)\n",
    "autoencoder_benign.fit(X_train_benign, X_train_benign, epochs=30, batch_size=32,verbose=1)\n",
    "autoencoder_malignant.fit(X_train_malignant, X_train_malignant, epochs=30, batch_size=32,verbose=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the custom loss fuction for the modified model\n",
    "from base64 import standard_b64decode\n",
    "\n",
    "\n",
    "def custom_loss_pos(X_batch, X_pred, labels_batch):\n",
    "    # Compute the MSE between the true input and the reconstructed input\n",
    "    mse = tf.reduce_mean(tf.square(X_batch - X_pred), axis=1)\n",
    "\n",
    "    # standard batch-level MSE\n",
    "    standard_batch_level_mse = tf.reduce_mean(mse)\n",
    "\n",
    "    # standard batch-level MSE using reduce_sum\n",
    "    standard_batch_level_mse_reduce_sum = tf.reduce_sum(mse) / len(labels_batch)\n",
    "\n",
    "    # Create a mask to include only those points where labels == 0\n",
    "    mask = tf.cast(labels_batch == 0, dtype=tf.float32)\n",
    "    mask = tf.reshape(mask, [-1])  # Flatten to shape [batch_size]\n",
    "\n",
    "    # Create the inverse mask\n",
    "    inverse_mask = 1 - mask\n",
    "\n",
    "    # Apply the mask to the MSE\n",
    "    masked_mse = mse * mask\n",
    "\n",
    "    # Apply the inverse mask to the MSE\n",
    "    inverse_masked_mse = mse * inverse_mask\n",
    "\n",
    "    # Calculate the masked batch-level MSE\n",
    "    masked_numerator = tf.reduce_sum(masked_mse)\n",
    "    masked_denominator = tf.reduce_sum(mask)\n",
    "    masked_batch_level_mse = masked_numerator / masked_denominator\n",
    "\n",
    "    # Calculate the inverse masked batch-level MSE\n",
    "    inverse_masked_numerator = tf.reduce_sum(inverse_masked_mse)\n",
    "    inverse_masked_denominator = tf.reduce_sum(inverse_mask)\n",
    "    inverse_masked_batch_level_mse = inverse_masked_numerator / inverse_masked_denominator\n",
    "    \n",
    "    \n",
    "    # # print debug information\n",
    "    # tf.print(\"-----------------------------\")\n",
    "    # tf.print(\"Standard batch-level MSE:\", standard_batch_level_mse)\n",
    "    # tf.print(\"Standard batch-level MSE using reduce_sum:\", standard_batch_level_mse_reduce_sum)\n",
    "    # tf.print(\"Mask length:\", tf.reduce_sum(mask))\n",
    "    # tf.print(\"Inverse mask length:\", tf.reduce_sum(inverse_mask))\n",
    "    # tf.print(\"n lables in batch:\", len(labels_batch))\n",
    "    # tf.print(\" \")\n",
    "    # tf.print(\"masked sum:\", tf.reduce_sum(masked_mse))\n",
    "    # tf.print(\"inverse masked sum:\", tf.reduce_sum(inverse_masked_mse))\n",
    "    # tf.print(\" \")\n",
    "    # tf.print(\"masked numerator:\", masked_numerator)\n",
    "    # tf.print(\"unmasked numerator:\", inverse_masked_numerator)\n",
    "    # tf.print(\"masked denominator:\", masked_denominator)\n",
    "    # tf.print(\"unmasked denominator:\", inverse_masked_denominator)\n",
    "    # tf.print(\"Masked batch level MSE:\", masked_batch_level_mse)\n",
    "    # tf.print(\"Inverse masked batch level MSE:\", inverse_masked_batch_level_mse)\n",
    "    # tf.print(\" \")\n",
    "    # tf.print(\"point level mse:\", mse)\n",
    "    # tf.print(\" \")\n",
    "    # tf.print(\"mse shape:\", mse.shape)\n",
    "    # tf.print(\"mask shape:\", mask.shape)\n",
    "    # tf.print(\"inverse mask shape:\", inverse_mask.shape)\n",
    "    # tf.print(\"mask\", mask)\n",
    "    # tf.print(\"inverse mask\", inverse_mask)\n",
    "    # tf.print(\"masked point level mse:\", masked_mse)\n",
    "    # tf.print(\"inverse masked point level mse:\", inverse_masked_mse)\n",
    "    \n",
    "    \n",
    "    # Return the masked MSE \n",
    "    return masked_batch_level_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert X_train and y_train to float32 if not already converted\n",
    "if X_train.dtype != tf.float32:\n",
    "    X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "\n",
    "if y_train.dtype != tf.float32:\n",
    "    y_train = tf.convert_to_tensor(y_train.values.reshape(-1, 1), dtype=tf.float32)\n",
    "\n",
    "# Create a dataset that includes features and labels\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "\n",
    "# Shuffle and batch the dataset\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Define the autoencoder model (make sure it also outputs float32)\n",
    "autoencoder_pos = create_autoencoder(input_dim)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Number of epochs to train\n",
    "num_epochs = 30\n",
    "\n",
    "# Custom training loop to replicate `model.fit`\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    # Iterate over the batches of the dataset\n",
    "    for step, (X_batch, labels_batch) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass: Compute predictions\n",
    "            X_pred = autoencoder_pos(X_batch, training=True)\n",
    "\n",
    "            # Compute the loss using the custom loss function\n",
    "            loss = custom_loss_pos(X_batch, X_pred, labels_batch)\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(loss, autoencoder_pos.trainable_weights)\n",
    "\n",
    "        # Apply gradients to update the model's weights\n",
    "        optimizer.apply_gradients(zip(gradients, autoencoder_pos.trainable_weights))\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}: Loss = {loss.numpy()}\")\n",
    "\n",
    "    # Print the loss for the epoch (optional, for monitoring)\n",
    "    print(f\"Epoch {epoch+1} completed with final batch loss: {loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get feature-level reconstruction errors for positive and negative autoencoder\n",
    "X_test_normal = autoencoder_normal.predict(X_test)\n",
    "X_test_benign = autoencoder_benign.predict(X_test)\n",
    "X_test_malignant = autoencoder_malignant.predict(X_test)\n",
    "X_test_pos = autoencoder_pos.predict(X_test)\n",
    "\n",
    "# get the patient-level (ie point-level) reconstruction errors (ie the scores)\n",
    "scores_normal = np.mean(np.power(X_test - X_test_normal, 2), axis=1)\n",
    "scores_benign = np.mean(np.power(X_test - X_test_benign, 2), axis=1)\n",
    "scores_malignant = np.mean(np.power(X_test - X_test_malignant, 2), axis=1)\n",
    "scores_pos = np.mean(np.power(X_test - X_test_pos, 2), axis=1)\n",
    "\n",
    "# Calculate the rocs and aucs\n",
    "fpr_normal, tpr_normal, _ = roc_curve(y_test, scores_normal)\n",
    "roc_auc_normal = auc(fpr_normal, tpr_normal)\n",
    "fpr_benign, tpr_benign, _ = roc_curve(y_test, scores_benign)\n",
    "roc_auc_benign = auc(fpr_benign, tpr_benign)\n",
    "fpr_malignant, tpr_malignant, _ = roc_curve(y_test, scores_malignant)\n",
    "roc_auc_malignant = auc(fpr_malignant, tpr_malignant)\n",
    "fpr_pos, tpr_pos, _ = roc_curve(y_test, scores_pos)\n",
    "roc_auc_pos = auc(fpr_pos, tpr_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the roc curves\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr_normal, tpr_normal, color='darkorange', lw=lw, label='Normal Autoencoder (area = %0.2f)' % roc_auc_normal)\n",
    "plt.plot(fpr_benign, tpr_benign, color='green', lw=lw, label='Benign Autoencoder (area = %0.2f)' % roc_auc_benign)\n",
    "plt.plot(fpr_malignant, tpr_malignant, color='red', lw=lw, label='Malignant Autoencoder (area = %0.2f)' % roc_auc_malignant)\n",
    "plt.plot(fpr_pos, tpr_pos, color='blue', lw=lw, label='Positive Autoencoder (area = %0.2f)' % roc_auc_pos)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
